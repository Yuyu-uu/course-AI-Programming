{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a6888b1",
   "metadata": {},
   "source": [
    "最终模型请查看最后一个单元格，     \n",
    "作业使用了deepseek-V3帮助理解代码和优化，    \n",
    "作业完整文件夹也上传了 https://github.com/Yuyu-uu/course-AI-Programming/tree/main/Homework/hw6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c229de4e",
   "metadata": {},
   "source": [
    "# 还是先仔细理解一下原来的代码：    \n",
    "\n",
    "先构建中英文原始语料文本;    \n",
    "再进行分词，并构建词汇表，给每个词分配一个唯一的索引，这样就将句子的文本数据转换为索引的数字序列。这个时候它是离散的向量形式，是模型的输入;   \n",
    "\n",
    "数字序列输入到模型后，先经过 Embedding Layer,嵌入层 `self.embedding = nn.Embedding(input_dim, d_model)`将输入的数字序列转换为维度为 d_model 的连续向量，在连续向量空间中，语义相近的词对应的向量在空间中距离较近；     \n",
    "\n",
    "在嵌入层之后，Transformer 模型通过多头自注意力机制和前馈神经网络对连续向量进行特征提取和转换。多头自注意力机制允许模型在处理每个位置的向量时，考虑序列中其他位置的信息，从而捕捉长距离的依赖关系。前馈神经网络则对自注意力机制的输出进行进一步的非线性变换，提取更高级的特征；      \n",
    "\n",
    "经过编码器和解码器的处理后，模型的输出是连续的向量。再通过一个全连接层`self.fc_out = nn.Linear(d_model, output_dim)）`将输出向量映射到词汇表的维度，得到每个词的概率分布。通过选择概率最大的词对应的索引，将连续向量转换回离散的数字序列，再转换回对应文本。\n",
    "\n",
    "（此外，因为 Transformer 模型本身不具备捕捉序列中位置信息的能力，所以需要额外的位置编码`def _generate_positional_encoding(self, seq_len)`）\n",
    " \n",
    "\n",
    "多头自注意力机制和前馈神经网络的具体操作见课件，直观的理解是，查询向量 qi 可以看作是当前词在寻找与之相关的信息，键向量 kj 可以看作是其他词提供的信息索引，值向量 vj 是其他词携带的具体信息。通过计算查询向量和键向量的点积 scorei,j 可以得到当前词与其他词的关联程度。然后，根据这个关联程度对值向量进行加权求和，就可以得到当前词在考虑其他词信息后的表示。\n",
    "\n",
    "\n",
    "# 模型目前的参数\n",
    "\n",
    "Transformer 层：`nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout, batch_first=False)`   \n",
    "D_MODEL：模型的隐藏层维度，值为 32    \n",
    "NHEAD：多头注意力机制的头数，值为 2    \n",
    "NUM_ENCODER_LAYERS：编码器的层数，值为 2    \n",
    "NUM_DECODER_LAYERS：解码器的层数，值为 2    \n",
    "DIM_FEEDFORWARD：前馈神经网络的隐藏层维度，值为 32    \n",
    "DROPOUT：Dropout 概率，值为 0.05   \n",
    "\n",
    "nn.Transformer 内部采用 ReLU 作为前馈神经网络的激活函数。\n",
    "\n",
    "损失函数：交叉熵损失：`nn.CrossEntropyLoss(ignore_index = 0)`  \n",
    "\n",
    "优化器: Adam 优化器 `torch.optim.Adam(model.parameters(), lr = 0.001)`，结合了 AdaGrad 和 RMSProp ，能够自适应地调整每个参数的学习率  \n",
    "\n",
    "训练参数：\n",
    "MAX_EPOCH：训练轮次，值为 200   \n",
    "batch_size：批大小，在 DataLoader 里设置为 8   \n",
    "lr：学习率，在 Adam 优化器中设置为 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d274b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d59abf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['你好', '今天 天气 很 好', '今天 天气 很 好', '我 爱 学习', '我 喜欢 狗', '天气 很 好', '我 爱 养猫', '我 喜欢 学习', '你好', '今天 天气 很 好', '爱 养猫今天', '天气', '很', '好', '我', '爱', '学习', '我', '喜欢', '狗', '猫']\n",
      "['Hello', 'today weather very good', 'today weather very good', 'I love learning', 'I like dog', 'weather very good', 'I love cat', 'I like study', 'Hello', 'today weather very good', 'love cattoday', 'weather', 'very', 'good', 'I', 'love', 'learning', 'I', 'like', 'dog', 'cat']\n"
     ]
    }
   ],
   "source": [
    "# 设置随机种子以确保可重复性\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 中文和英文句子\n",
    "chinese_sentences = [ \"你好\", \"今天 天气 很 好\",\n",
    "                     \"今天 天气 很 好\",\n",
    "                     \"我 爱 学习\",\"我 喜欢 狗\",\n",
    "                     \"天气 很 好\",\"我 爱 养猫\",\"我 喜欢 学习\",\n",
    "                     \"你好\", \"今天 天气 很 好\",\"爱 养猫\"\n",
    "                     \"今天\", \"天气\", \"很\", \"好\",\n",
    "                     \"我\", \"爱\", \"学习\",\"我\",\"喜欢\",\"狗\",\"猫\",\n",
    "                     ]\n",
    "english_sentences = [ \"Hello\", \"today weather very good\",\n",
    "                     \"today weather very good\",\n",
    "                     \"I love learning\",\"I like dog\",\n",
    "                     \"weather very good\",\"I love cat\",\"I like study\",\n",
    "                     \"Hello\", \"today weather very good\",\"love cat\"\n",
    "                     \"today\", \"weather\", \"very\", \"good\",\n",
    "                     \"I\", \"love\", \"learning\",\"I\",\"like\",\"dog\",\"cat\",\n",
    "                     ]\n",
    "\n",
    "print(chinese_sentences)\n",
    "print(english_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6ab035f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'你好': 4, '今天': 5, '天气': 6, '很': 7, '好': 8, '我': 9, '爱': 10, '学习': 11, '喜欢': 12, '狗': 13, '养': 14, '猫': 15, '养猫': 16, '<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
      "{'Hello': 4, 'today': 5, 'weather': 6, 'very': 7, 'good': 8, 'I': 9, 'love': 10, 'learning': 11, 'like': 12, 'dog': 13, 'cat': 14, 'study': 15, 'cattoday': 16, '<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
      "[([1, 4, 2], [1, 4, 2]), ([1, 5, 6, 7, 8, 2], [1, 5, 6, 7, 8, 2]), ([1, 5, 6, 7, 8, 2], [1, 5, 6, 7, 8, 2]), ([1, 9, 10, 11, 2], [1, 9, 10, 11, 2]), ([1, 9, 12, 13, 2], [1, 9, 12, 13, 2]), ([1, 6, 7, 8, 2], [1, 6, 7, 8, 2]), ([1, 9, 10, 14, 2], [1, 9, 10, 14, 15, 2]), ([1, 9, 12, 15, 2], [1, 9, 12, 11, 2]), ([1, 4, 2], [1, 4, 2]), ([1, 5, 6, 7, 8, 2], [1, 5, 6, 7, 8, 2]), ([1, 10, 16, 2], [1, 10, 16, 5, 2]), ([1, 6, 2], [1, 6, 2]), ([1, 7, 2], [1, 7, 2]), ([1, 8, 2], [1, 8, 2]), ([1, 9, 2], [1, 9, 2]), ([1, 10, 2], [1, 10, 2]), ([1, 11, 2], [1, 11, 2]), ([1, 9, 2], [1, 9, 2]), ([1, 12, 2], [1, 12, 2]), ([1, 13, 2], [1, 13, 2]), ([1, 14, 2], [1, 15, 2])]\n"
     ]
    }
   ],
   "source": [
    "# 加载 spacy 分词器\n",
    "spacy_ch = spacy.load('zh_core_web_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "# 分词函数\n",
    "def tokenize_ch(text):\n",
    "    return [tok.text for tok in spacy_ch.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# 构建词汇表\n",
    "\n",
    "def build_vocab(data, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for tokens in data:\n",
    "        counter.update(tokens)\n",
    "    vocab = {word: idx + 4 for idx, (word, freq) in enumerate(counter.items()) if freq >= min_freq}\n",
    "    vocab['<pad>'] = 0\n",
    "    vocab['<sos>'] = 1\n",
    "    vocab['<eos>'] = 2\n",
    "    vocab['<unk>'] = 3\n",
    "    return vocab\n",
    "\n",
    "# 构建中文和英文词汇表\n",
    "chinese_vocab = build_vocab([tokenize_ch(s) for s in chinese_sentences])\n",
    "english_vocab = build_vocab([tokenize_en(s) for s in english_sentences])\n",
    "\n",
    "def sentence_to_indices(sentence, vocab):\n",
    "    return [vocab['<sos>']] + [vocab.get(word, vocab['<unk>']) for word in sentence] + [vocab['<eos>']]\n",
    "\n",
    "# 将句子转换为索引序列\n",
    "data = [\n",
    "    (sentence_to_indices(tokenize_en(english), english_vocab),  # 英文句子和英文词汇表\n",
    "    sentence_to_indices(tokenize_ch(chinese), chinese_vocab)   # 中文句子和中文词汇表\n",
    "    )\n",
    "    for chinese, english in zip(chinese_sentences, english_sentences)\n",
    "]\n",
    "print(chinese_vocab)\n",
    "print(english_vocab)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6c2b7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据整理函数\n",
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    src_pad = pad_sequence([torch.tensor(s) for s in src_batch], padding_value=0, batch_first=False)\n",
    "    trg_pad = pad_sequence([torch.tensor(t) for t in trg_batch], padding_value=0, batch_first=False)\n",
    "    return src_pad, trg_pad\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "dataset = TranslationDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd8e2192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer模型（修改输入输出维度的含义，代码结构不变）\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout, batch_first=False)\n",
    "        self.fc_out = nn.Linear(d_model, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def _generate_positional_encoding(self, seq_len):\n",
    "        position = torch.arange(seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2, dtype=torch.float) * (-math.log(10000.0) / self.d_model))\n",
    "        pe = torch.zeros(seq_len, self.d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(1)  # [1, seq_len, d_model]\n",
    "\n",
    "    def forward(self, src, trg, trg_mask=None, padding_mask=None):\n",
    "        src_seq_length, N = src.shape\n",
    "        trg_seq_length, N = trg.shape\n",
    "        # 动态生成位置编码\n",
    "        src_pos = self._generate_positional_encoding(src_seq_length).to(src.device)\n",
    "        trg_pos = self._generate_positional_encoding(trg_seq_length).to(trg.device)\n",
    "        # 扩展位置编码的形状以匹配输入\n",
    "        src_pos = src_pos.expand(-1, N, -1)  # [1, seq_len, d_model] -> [seq_len, batch_size, d_model]\n",
    "        trg_pos = trg_pos.expand(-1, N, -1)  # [1, seq_len, d_model] -> [seq_len, batch_size, d_model]\n",
    "        \n",
    "        src = self.dropout(self.embedding(src) + src_pos)\n",
    "        trg = self.dropout(self.embedding(trg) + trg_pos)\n",
    "        if trg_mask is None:\n",
    "            output = self.transformer(src, trg)\n",
    "        else:\n",
    "            output = self.transformer(src, trg, tgt_mask=trg_mask, tgt_key_padding_mask=padding_mask)\n",
    "        prediction = self.fc_out(output)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae48591a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python 3.11.5\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 0.046130478382110596\n",
      "Epoch 200, Loss: 0.011155758053064346\n",
      "Training Finished\n"
     ]
    }
   ],
   "source": [
    "# 调整输入输出维度（源语言英文词汇表长度→输入，目标语言中文词汇表长度→输出）\n",
    "INPUT_DIM = len(english_vocab)  # 源语言（英文）词汇表大小\n",
    "OUTPUT_DIM = len(chinese_vocab)  # 目标语言（中文）词汇表大小\n",
    "D_MODEL = 32\n",
    "NHEAD = 2\n",
    "NUM_ENCODER_LAYERS = 2\n",
    "NUM_DECODER_LAYERS = 2\n",
    "DIM_FEEDFORWARD = 32\n",
    "DROPOUT = 0.05\n",
    "MAX_EPOCH = 200\n",
    "\n",
    "model = Transformer(INPUT_DIM, OUTPUT_DIM, D_MODEL, NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, DIM_FEEDFORWARD, DROPOUT)\n",
    "# print(model)\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    for i, (src, trg) in enumerate(dataloader):\n",
    "        trg_mask = nn.Transformer.generate_square_subsequent_mask(trg.size(0)-1).bool()\n",
    "        padding_mask = (trg[1:,] == 0).transpose(0, 1)\n",
    "        output = model(src, trg[:-1,], trg_mask=trg_mask, padding_mask=padding_mask)\n",
    "        loss = criterion(output.view(-1, OUTPUT_DIM), trg[1:,].view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 99:     \n",
    "        print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7425de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model have saved to ./model/mymodel_en2zh.pth\n"
     ]
    }
   ],
   "source": [
    "# 保存模型\n",
    "model_save_path = \"./model/mymodel_en2zh.pth\"\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model have saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13e3df4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Hello -> Translated: 你好\n",
      "Input: today weather very good -> Translated: 今天 天气 很 好\n",
      "Input: I love dog -> Translated: 我 爱 狗\n",
      "Input: I like cat -> Translated: 我 喜欢 狗\n",
      "Input: love cat -> Translated: 爱 猫\n"
     ]
    }
   ],
   "source": [
    "# 翻译函数\n",
    "def translate_sentence(sentence, src_vocab, trg_vocab, model, max_len=50):\n",
    "    model.eval()\n",
    "    # 更改分词函数\n",
    "    tokens = tokenize_en(sentence)\n",
    "    indices = sentence_to_indices(tokens, src_vocab)    \n",
    "    src_tensor = torch.tensor(indices).unsqueeze(1)\n",
    "    #src_len=torch.tensor(len(indices)).unsqueeze(0)\n",
    "    #print(\"src_tensor:\",src_tensor)\n",
    "    trg_indices = [trg_vocab['<sos>']]  # 目标语言以<sos>开始\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.tensor(trg_indices).unsqueeze(1)\n",
    "        with torch.no_grad():\n",
    "            output = model(src_tensor, trg_tensor)\n",
    "        pred_token = output.argmax(2)[-1].item()\n",
    "        trg_indices.append(pred_token)\n",
    "        if pred_token == trg_vocab['<eos>']:\n",
    "            break\n",
    "    \n",
    "    # 转换为目标语言（中文）tokens\n",
    "    trg_tokens = [list(trg_vocab.keys())[list(trg_vocab.values()).index(i)] for i in trg_indices]\n",
    "    final_tokens = [token for token in trg_tokens if token not in ['<sos>', '<eos>']]\n",
    "    return ' '.join(final_tokens)\n",
    "\n",
    "# 加载模型（结构不变）\n",
    "loaded_model = Transformer(INPUT_DIM, OUTPUT_DIM, D_MODEL, NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, DIM_FEEDFORWARD, DROPOUT)\n",
    "loaded_model.load_state_dict(torch.load(model_save_path, weights_only=True))\n",
    "\n",
    "# 测试翻译（输入英文句子）\n",
    "english_test_sentences = [\"Hello\", \"today weather very good\", \"I love dog\", \"I like cat\",\"love cat\"]\n",
    "for sentence in english_test_sentences:\n",
    "    translation = translate_sentence(sentence, english_vocab, chinese_vocab, loaded_model)\n",
    "    print(f'Input: {sentence} -> Translated: {translation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dd12e3",
   "metadata": {},
   "source": [
    "以上将输入变为英文，输出变为中文，其他结构不变，实现英译中。但可以看到，翻译结果并不都正确，语料库中出现的原例都可以翻译正确，但比如 “like cat” 就翻译错了，“like” 和 “cat” 同时出现的情况没有学习过。\n",
    "\n",
    "在不更改语料库的情况下，我们尝试调整网络结构，再尝试不同的激活函数、损失函数，再探究合适的训练轮次和更优的学习率调整策略，最后扫描一下训练参数网格，得到最适合这个任务的参数组合。\n",
    "\n",
    "（虽然感觉这个问题主要还是语料库太小，“like”后面只出现过“dog”，所以学习到的模型“like”和“dog”关系会很密切，查询向量和键向量的点积会比较大。而且如果要的是这种泛化能力，即翻译未出现过的词汇组合，调整代码使这个loss最小并不一定可以实现这个目的，不过这个泛化能力的标准也没法准确描述，也许可以让它翻译多个未出现的组合，看哪种参数组合翻译对的多？不过这么小的语料库这样比较也不够科学）\n",
    "以下还是以Loss最小为优劣标准，即训练模型的目标是将语料库的原例尽量翻译准确。\n",
    "\n",
    "以下先尝试不同的网络结构："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed0c15b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'D_MODEL': 16, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 16}, Loss: 0.9271707016353806\n",
      "Parameters: {'D_MODEL': 16, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 32}, Loss: 0.7484967168420553\n",
      "Parameters: {'D_MODEL': 16, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.6622638514513771\n",
      "Parameters: {'D_MODEL': 16, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.6622638514513771\n",
      "Parameters: {'D_MODEL': 16, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.6622638514513771\n",
      "Parameters: {'D_MODEL': 16, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 3, 'DIM_FEEDFORWARD': 64}, Loss: 0.6528403010840217\n",
      "Parameters: {'D_MODEL': 16, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 3, 'DIM_FEEDFORWARD': 64}, Loss: 0.6528403010840217\n",
      "Parameters: {'D_MODEL': 16, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 3, 'DIM_FEEDFORWARD': 64}, Loss: 0.6528403010840217\n",
      "Parameters: {'D_MODEL': 16, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 3, 'DIM_FEEDFORWARD': 64}, Loss: 0.6528403010840217\n",
      "Parameters: {'D_MODEL': 16, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 3, 'DIM_FEEDFORWARD': 64}, Loss: 0.6528403010840217\n",
      "Parameters: {'D_MODEL': 16, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 3, 'DIM_FEEDFORWARD': 64}, Loss: 0.6528403010840217\n",
      "Parameters: {'D_MODEL': 16, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 3, 'NUM_DECODER_LAYERS': 3, 'DIM_FEEDFORWARD': 64}, Loss: 0.6302607794851065\n",
      "Parameters: {'D_MODEL': 16, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 3, 'NUM_DECODER_LAYERS': 3, 'DIM_FEEDFORWARD': 64}, Loss: 0.6302607794851065\n",
      "Parameters: {'D_MODEL': 16, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 3, 'NUM_DECODER_LAYERS': 3, 'DIM_FEEDFORWARD': 64}, Loss: 0.6302607794851065\n",
      "Parameters: {'D_MODEL': 16, 'NHEAD': 4, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.624583710146447\n",
      "Parameters: {'D_MODEL': 16, 'NHEAD': 4, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.624583710146447\n",
      "Parameters: {'D_MODEL': 16, 'NHEAD': 4, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.624583710146447\n",
      "Parameters: {'D_MODEL': 16, 'NHEAD': 4, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.624583710146447\n",
      "Parameters: {'D_MODEL': 16, 'NHEAD': 4, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.624583710146447\n",
      "Parameters: {'D_MODEL': 16, 'NHEAD': 4, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.624583710146447\n",
      "Parameters: {'D_MODEL': 16, 'NHEAD': 4, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.624583710146447\n",
      "Parameters: {'D_MODEL': 16, 'NHEAD': 4, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.624583710146447\n",
      "Parameters: {'D_MODEL': 16, 'NHEAD': 4, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.624583710146447\n",
      "Parameters: {'D_MODEL': 16, 'NHEAD': 4, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.624583710146447\n",
      "Parameters: {'D_MODEL': 32, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 16}, Loss: 0.373122432862098\n",
      "Parameters: {'D_MODEL': 32, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 32}, Loss: 0.3367406509118155\n",
      "Parameters: {'D_MODEL': 32, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.32164853119601805\n",
      "Parameters: {'D_MODEL': 32, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.32164853119601805\n",
      "Parameters: {'D_MODEL': 32, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.32164853119601805\n",
      "Parameters: {'D_MODEL': 32, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 3, 'DIM_FEEDFORWARD': 64}, Loss: 0.3166973998754596\n",
      "Parameters: {'D_MODEL': 32, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 3, 'DIM_FEEDFORWARD': 64}, Loss: 0.3166973998754596\n",
      "Parameters: {'D_MODEL': 32, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 3, 'DIM_FEEDFORWARD': 64}, Loss: 0.3166973998754596\n",
      "Parameters: {'D_MODEL': 32, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 3, 'DIM_FEEDFORWARD': 64}, Loss: 0.3166973998754596\n",
      "Parameters: {'D_MODEL': 32, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 3, 'DIM_FEEDFORWARD': 64}, Loss: 0.3166973998754596\n",
      "Parameters: {'D_MODEL': 32, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 3, 'DIM_FEEDFORWARD': 64}, Loss: 0.3166973998754596\n",
      "Parameters: {'D_MODEL': 32, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 3, 'NUM_DECODER_LAYERS': 3, 'DIM_FEEDFORWARD': 64}, Loss: 0.30158085936990875\n",
      "Parameters: {'D_MODEL': 32, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 3, 'NUM_DECODER_LAYERS': 3, 'DIM_FEEDFORWARD': 64}, Loss: 0.30158085936990875\n",
      "Parameters: {'D_MODEL': 32, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 3, 'NUM_DECODER_LAYERS': 3, 'DIM_FEEDFORWARD': 64}, Loss: 0.30158085936990875\n",
      "Parameters: {'D_MODEL': 32, 'NHEAD': 4, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.2778240911553924\n",
      "Parameters: {'D_MODEL': 32, 'NHEAD': 4, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.2778240911553924\n",
      "Parameters: {'D_MODEL': 32, 'NHEAD': 4, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.2778240911553924\n",
      "Parameters: {'D_MODEL': 32, 'NHEAD': 4, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.2778240911553924\n",
      "Parameters: {'D_MODEL': 32, 'NHEAD': 4, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.2778240911553924\n",
      "Parameters: {'D_MODEL': 32, 'NHEAD': 4, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.2778240911553924\n",
      "Parameters: {'D_MODEL': 32, 'NHEAD': 4, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.2778240911553924\n",
      "Parameters: {'D_MODEL': 32, 'NHEAD': 4, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.2778240911553924\n",
      "Parameters: {'D_MODEL': 32, 'NHEAD': 4, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.2778240911553924\n",
      "Parameters: {'D_MODEL': 32, 'NHEAD': 4, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.2778240911553924\n",
      "Parameters: {'D_MODEL': 64, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 16}, Loss: 0.1607974853711979\n",
      "Parameters: {'D_MODEL': 64, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 16}, Loss: 0.1607974853711979\n",
      "Parameters: {'D_MODEL': 64, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.1533485722479721\n",
      "Parameters: {'D_MODEL': 64, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.1533485722479721\n",
      "Parameters: {'D_MODEL': 64, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.1533485722479721\n",
      "Parameters: {'D_MODEL': 64, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.1533485722479721\n",
      "Parameters: {'D_MODEL': 64, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.1533485722479721\n",
      "Parameters: {'D_MODEL': 64, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.1533485722479721\n",
      "Parameters: {'D_MODEL': 64, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.1533485722479721\n",
      "Parameters: {'D_MODEL': 64, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.1533485722479721\n",
      "Parameters: {'D_MODEL': 64, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.1533485722479721\n",
      "Parameters: {'D_MODEL': 64, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.1533485722479721\n",
      "Parameters: {'D_MODEL': 64, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.1533485722479721\n",
      "Parameters: {'D_MODEL': 64, 'NHEAD': 2, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.1533485722479721\n",
      "Parameters: {'D_MODEL': 64, 'NHEAD': 4, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.14150979904807173\n",
      "Parameters: {'D_MODEL': 64, 'NHEAD': 4, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.14150979904807173\n",
      "Parameters: {'D_MODEL': 64, 'NHEAD': 4, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.14150979904807173\n",
      "Parameters: {'D_MODEL': 64, 'NHEAD': 4, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.14150979904807173\n",
      "Parameters: {'D_MODEL': 64, 'NHEAD': 4, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.14150979904807173\n",
      "Parameters: {'D_MODEL': 64, 'NHEAD': 4, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.14150979904807173\n",
      "Parameters: {'D_MODEL': 64, 'NHEAD': 4, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.14150979904807173\n",
      "Parameters: {'D_MODEL': 64, 'NHEAD': 4, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.14150979904807173\n",
      "Parameters: {'D_MODEL': 64, 'NHEAD': 4, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.14150979904807173\n",
      "Parameters: {'D_MODEL': 64, 'NHEAD': 4, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Loss: 0.14150979904807173\n",
      "Best parameters: {'D_MODEL': 64, 'NHEAD': 4, 'NUM_ENCODER_LAYERS': 2, 'NUM_DECODER_LAYERS': 2, 'DIM_FEEDFORWARD': 64}, Best loss: 0.14150979904807173\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import spacy\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n",
    "\n",
    "# 设置随机种子以确保可重复性\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 中文和英文句子\n",
    "chinese_sentences = [\n",
    "    \"你好\", \"今天 天气 很 好\",\n",
    "    \"今天 天气 很 好\",\n",
    "    \"我 爱 学习\", \"我 喜欢 狗\",\n",
    "    \"天气 很 好\", \"我 爱 养猫\", \"我 喜欢 学习\",\n",
    "    \"你好\", \"今天 天气 很 好\", \"爱 养猫\",\n",
    "    \"今天\", \"天气\", \"很\", \"好\",\n",
    "    \"我\", \"爱\", \"学习\", \"我\", \"喜欢\", \"狗\", \"猫\",\n",
    "]\n",
    "english_sentences = [\n",
    "    \"Hello\", \"today weather very good\",\n",
    "    \"today weather very good\",\n",
    "    \"I love learning\", \"I like dog\",\n",
    "    \"weather very good\", \"I love cat\", \"I like study\",\n",
    "    \"Hello\", \"today weather very good\", \"love cat\",\n",
    "    \"today\", \"weather\", \"very\", \"good\",\n",
    "    \"I\", \"love\", \"learning\", \"I\", \"like\", \"dog\", \"cat\",\n",
    "]\n",
    "\n",
    "# 加载 spacy 分词器\n",
    "spacy_ch = spacy.load('zh_core_web_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "# 分词函数\n",
    "def tokenize_ch(text):\n",
    "    return [tok.text for tok in spacy_ch.tokenizer(text)]\n",
    "\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "\n",
    "# 构建词汇表\n",
    "def build_vocab(data, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for tokens in data:\n",
    "        counter.update(tokens)\n",
    "    vocab = {word: idx + 4 for idx, (word, freq) in enumerate(counter.items()) if freq >= min_freq}\n",
    "    vocab['<pad>'] = 0\n",
    "    vocab['<sos>'] = 1\n",
    "    vocab['<eos>'] = 2\n",
    "    vocab['<unk>'] = 3\n",
    "    return vocab\n",
    "\n",
    "\n",
    "# 构建中文和英文词汇表\n",
    "chinese_vocab = build_vocab([tokenize_ch(s) for s in chinese_sentences])\n",
    "english_vocab = build_vocab([tokenize_en(s) for s in english_sentences])\n",
    "\n",
    "\n",
    "def sentence_to_indices(sentence, vocab):\n",
    "    return [vocab['<sos>']] + [vocab.get(word, vocab['<unk>']) for word in sentence] + [vocab['<eos>']]\n",
    "\n",
    "\n",
    "# 将句子转换为索引序列\n",
    "data = [\n",
    "    (sentence_to_indices(tokenize_en(english), english_vocab),  # 英文句子和英文词汇表\n",
    "     sentence_to_indices(tokenize_ch(chinese), chinese_vocab)  # 中文句子和中文词汇表\n",
    "     )\n",
    "    for chinese, english in zip(chinese_sentences, english_sentences)\n",
    "]\n",
    "\n",
    "# 数据整理函数\n",
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    src_pad = pad_sequence([torch.tensor(s) for s in src_batch], padding_value=0, batch_first=False)\n",
    "    trg_pad = pad_sequence([torch.tensor(t) for t in trg_batch], padding_value=0, batch_first=False)\n",
    "    return src_pad, trg_pad\n",
    "\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "dataset = TranslationDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Transformer模型（修改输入输出维度的含义，代码结构不变）\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward,\n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward,\n",
    "                                          dropout, batch_first=False)\n",
    "        self.fc_out = nn.Linear(d_model, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _generate_positional_encoding(self, seq_len):\n",
    "        position = torch.arange(seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2, dtype=torch.float) * (-math.log(10000.0) / self.d_model))\n",
    "        pe = torch.zeros(seq_len, self.d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(1)  # [1, seq_len, d_model]\n",
    "\n",
    "    def forward(self, src, trg, trg_mask=None, padding_mask=None):\n",
    "        src_seq_length, N = src.shape\n",
    "        trg_seq_length, N = trg.shape\n",
    "        # 动态生成位置编码\n",
    "        src_pos = self._generate_positional_encoding(src_seq_length).to(src.device)\n",
    "        trg_pos = self._generate_positional_encoding(trg_seq_length).to(trg.device)\n",
    "        # 扩展位置编码的形状以匹配输入\n",
    "        src_pos = src_pos.expand(-1, N, -1)  # [1, seq_len, d_model] -> [seq_len, batch_size, d_model]\n",
    "        trg_pos = trg_pos.expand(-1, N, -1)  # [1, seq_len, d_model] -> [seq_len, batch_size, d_model]\n",
    "\n",
    "        src = self.dropout(self.embedding(src) + src_pos)\n",
    "        trg = self.dropout(self.embedding(trg) + trg_pos)\n",
    "        if trg_mask is None:\n",
    "            output = self.transformer(src, trg)\n",
    "        else:\n",
    "            output = self.transformer(src, trg, tgt_mask=trg_mask, tgt_key_padding_mask=padding_mask)\n",
    "        prediction = self.fc_out(output)\n",
    "        return prediction\n",
    "\n",
    "\n",
    "# 定义要搜索的参数网格\n",
    "D_MODEL_LIST = [16, 32, 64]\n",
    "NHEAD_LIST = [2, 4]\n",
    "NUM_ENCODER_LAYERS_LIST = [2, 3]\n",
    "NUM_DECODER_LAYERS_LIST = [2, 3]\n",
    "DIM_FEEDFORWARD_LIST = [16, 32, 64]\n",
    "DROPOUT = 0.05\n",
    "MAX_EPOCH = 200\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "# 网格搜索\n",
    "for D_MODEL in D_MODEL_LIST:\n",
    "    for NHEAD in NHEAD_LIST:\n",
    "        for NUM_ENCODER_LAYERS in NUM_ENCODER_LAYERS_LIST:\n",
    "            for NUM_DECODER_LAYERS in NUM_DECODER_LAYERS_LIST:\n",
    "                for DIM_FEEDFORWARD in DIM_FEEDFORWARD_LIST:\n",
    "                    INPUT_DIM = len(english_vocab)  # 源语言（英文）词汇表大小\n",
    "                    OUTPUT_DIM = len(chinese_vocab)  # 目标语言（中文）词汇表大小\n",
    "\n",
    "                    model = Transformer(INPUT_DIM, OUTPUT_DIM, D_MODEL, NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS,\n",
    "                                        DIM_FEEDFORWARD, DROPOUT)\n",
    "                    # 定义损失函数和优化器\n",
    "                    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "                    total_loss = 0\n",
    "                    for epoch in range(MAX_EPOCH):\n",
    "                        for i, (src, trg) in enumerate(dataloader):\n",
    "                            trg_mask = nn.Transformer.generate_square_subsequent_mask(trg.size(0) - 1).bool()\n",
    "                            padding_mask = (trg[1:,] == 0).transpose(0, 1)\n",
    "                            output = model(src, trg[:-1,], trg_mask=trg_mask, padding_mask=padding_mask)\n",
    "                            loss = criterion(output.view(-1, OUTPUT_DIM), trg[1:,].view(-1))\n",
    "                            optimizer.zero_grad()\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                            total_loss += loss.item()\n",
    "\n",
    "                    avg_loss = total_loss / (MAX_EPOCH * len(dataloader))\n",
    "                    if avg_loss < best_loss:\n",
    "                        best_loss = avg_loss\n",
    "                        best_params = {\n",
    "                            'D_MODEL': D_MODEL,\n",
    "                            'NHEAD': NHEAD,\n",
    "                            'NUM_ENCODER_LAYERS': NUM_ENCODER_LAYERS,\n",
    "                            'NUM_DECODER_LAYERS': NUM_DECODER_LAYERS,\n",
    "                            'DIM_FEEDFORWARD': DIM_FEEDFORWARD\n",
    "                        }\n",
    "                    print(f\"Parameters: {best_params}, Loss: {best_loss}\")\n",
    "\n",
    "print(f\"Best parameters: {best_params}, Best loss: {best_loss}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75603e6",
   "metadata": {},
   "source": [
    "使用以上最优参数组合："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d771d124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['你好', '今天 天气 很 好', '今天 天气 很 好', '我 爱 学习', '我 喜欢 狗', '天气 很 好', '我 爱 养猫', '我 喜欢 学习', '你好', '今天 天气 很 好', '爱 养猫今天', '天气', '很', '好', '我', '爱', '学习', '我', '喜欢', '狗', '猫']\n",
      "['Hello', 'today weather very good', 'today weather very good', 'I love learning', 'I like dog', 'weather very good', 'I love cat', 'I like study', 'Hello', 'today weather very good', 'love cattoday', 'weather', 'very', 'good', 'I', 'love', 'learning', 'I', 'like', 'dog', 'cat']\n",
      "{'你好': 4, '今天': 5, '天气': 6, '很': 7, '好': 8, '我': 9, '爱': 10, '学习': 11, '喜欢': 12, '狗': 13, '养': 14, '猫': 15, '养猫': 16, '<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
      "{'Hello': 4, 'today': 5, 'weather': 6, 'very': 7, 'good': 8, 'I': 9, 'love': 10, 'learning': 11, 'like': 12, 'dog': 13, 'cat': 14, 'study': 15, 'cattoday': 16, '<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
      "[([1, 4, 2], [1, 4, 2]), ([1, 5, 6, 7, 8, 2], [1, 5, 6, 7, 8, 2]), ([1, 5, 6, 7, 8, 2], [1, 5, 6, 7, 8, 2]), ([1, 9, 10, 11, 2], [1, 9, 10, 11, 2]), ([1, 9, 12, 13, 2], [1, 9, 12, 13, 2]), ([1, 6, 7, 8, 2], [1, 6, 7, 8, 2]), ([1, 9, 10, 14, 2], [1, 9, 10, 14, 15, 2]), ([1, 9, 12, 15, 2], [1, 9, 12, 11, 2]), ([1, 4, 2], [1, 4, 2]), ([1, 5, 6, 7, 8, 2], [1, 5, 6, 7, 8, 2]), ([1, 10, 16, 2], [1, 10, 16, 5, 2]), ([1, 6, 2], [1, 6, 2]), ([1, 7, 2], [1, 7, 2]), ([1, 8, 2], [1, 8, 2]), ([1, 9, 2], [1, 9, 2]), ([1, 10, 2], [1, 10, 2]), ([1, 11, 2], [1, 11, 2]), ([1, 9, 2], [1, 9, 2]), ([1, 12, 2], [1, 12, 2]), ([1, 13, 2], [1, 13, 2]), ([1, 14, 2], [1, 15, 2])]\n",
      "Epoch 100, Loss: 0.012548543512821198\n",
      "Epoch 200, Loss: 0.0033949718344956636\n",
      "Training Finished\n",
      "Model have saved to ./model/mymodel_en2zh.pth\n",
      "Input: Hello -> Translated: 你好\n",
      "Input: today weather very good -> Translated: 今天 天气 很 好\n",
      "Input: I love dog -> Translated: 我 爱 养 狗\n",
      "Input: I like cat -> Translated: 我 喜欢 猫\n",
      "Input: love cat -> Translated: 爱 养 猫\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import spacy\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# 设置随机种子以确保可重复性\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 中文和英文句子\n",
    "chinese_sentences = [ \"你好\", \"今天 天气 很 好\",\n",
    "                     \"今天 天气 很 好\",\n",
    "                     \"我 爱 学习\",\"我 喜欢 狗\",\n",
    "                     \"天气 很 好\",\"我 爱 养猫\",\"我 喜欢 学习\",\n",
    "                     \"你好\", \"今天 天气 很 好\",\"爱 养猫\"\n",
    "                     \"今天\", \"天气\", \"很\", \"好\",\n",
    "                     \"我\", \"爱\", \"学习\",\"我\",\"喜欢\",\"狗\",\"猫\",\n",
    "                     ]\n",
    "english_sentences = [ \"Hello\", \"today weather very good\",\n",
    "                     \"today weather very good\",\n",
    "                     \"I love learning\",\"I like dog\",\n",
    "                     \"weather very good\",\"I love cat\",\"I like study\",\n",
    "                     \"Hello\", \"today weather very good\",\"love cat\"\n",
    "                     \"today\", \"weather\", \"very\", \"good\",\n",
    "                     \"I\", \"love\", \"learning\",\"I\",\"like\",\"dog\",\"cat\",\n",
    "                     ]\n",
    "\n",
    "print(chinese_sentences)\n",
    "print(english_sentences)\n",
    "\n",
    "\n",
    "# 加载 spacy 分词器\n",
    "spacy_ch = spacy.load('zh_core_web_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "# 分词函数\n",
    "def tokenize_ch(text):\n",
    "    return [tok.text for tok in spacy_ch.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# 构建词汇表\n",
    "\n",
    "def build_vocab(data, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for tokens in data:\n",
    "        counter.update(tokens)\n",
    "    vocab = {word: idx + 4 for idx, (word, freq) in enumerate(counter.items()) if freq >= min_freq}\n",
    "    vocab['<pad>'] = 0\n",
    "    vocab['<sos>'] = 1\n",
    "    vocab['<eos>'] = 2\n",
    "    vocab['<unk>'] = 3\n",
    "    return vocab\n",
    "\n",
    "# 构建中文和英文词汇表\n",
    "chinese_vocab = build_vocab([tokenize_ch(s) for s in chinese_sentences])\n",
    "english_vocab = build_vocab([tokenize_en(s) for s in english_sentences])\n",
    "\n",
    "def sentence_to_indices(sentence, vocab):\n",
    "    return [vocab['<sos>']] + [vocab.get(word, vocab['<unk>']) for word in sentence] + [vocab['<eos>']]\n",
    "\n",
    "# 将句子转换为索引序列\n",
    "data = [\n",
    "    (sentence_to_indices(tokenize_en(english), english_vocab),  # 英文句子和英文词汇表\n",
    "    sentence_to_indices(tokenize_ch(chinese), chinese_vocab)   # 中文句子和中文词汇表\n",
    "    )\n",
    "    for chinese, english in zip(chinese_sentences, english_sentences)\n",
    "]\n",
    "print(chinese_vocab)\n",
    "print(english_vocab)\n",
    "print(data)\n",
    "\n",
    "\n",
    "# 数据整理函数\n",
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    src_pad = pad_sequence([torch.tensor(s) for s in src_batch], padding_value=0, batch_first=False)\n",
    "    trg_pad = pad_sequence([torch.tensor(t) for t in trg_batch], padding_value=0, batch_first=False)\n",
    "    return src_pad, trg_pad\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "dataset = TranslationDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Transformer模型（修改输入输出维度的含义，代码结构不变）\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout, batch_first=False)\n",
    "        self.fc_out = nn.Linear(d_model, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def _generate_positional_encoding(self, seq_len):\n",
    "        position = torch.arange(seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2, dtype=torch.float) * (-math.log(10000.0) / self.d_model))\n",
    "        pe = torch.zeros(seq_len, self.d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(1)  # [1, seq_len, d_model]\n",
    "\n",
    "    def forward(self, src, trg, trg_mask=None, padding_mask=None):\n",
    "        src_seq_length, N = src.shape\n",
    "        trg_seq_length, N = trg.shape\n",
    "        # 动态生成位置编码\n",
    "        src_pos = self._generate_positional_encoding(src_seq_length).to(src.device)\n",
    "        trg_pos = self._generate_positional_encoding(trg_seq_length).to(trg.device)\n",
    "        # 扩展位置编码的形状以匹配输入\n",
    "        src_pos = src_pos.expand(-1, N, -1)  # [1, seq_len, d_model] -> [seq_len, batch_size, d_model]\n",
    "        trg_pos = trg_pos.expand(-1, N, -1)  # [1, seq_len, d_model] -> [seq_len, batch_size, d_model]\n",
    "        \n",
    "        src = self.dropout(self.embedding(src) + src_pos)\n",
    "        trg = self.dropout(self.embedding(trg) + trg_pos)\n",
    "        if trg_mask is None:\n",
    "            output = self.transformer(src, trg)\n",
    "        else:\n",
    "            output = self.transformer(src, trg, tgt_mask=trg_mask, tgt_key_padding_mask=padding_mask)\n",
    "        prediction = self.fc_out(output)\n",
    "        return prediction\n",
    "\n",
    "\n",
    "# 调整输入输出维度（源语言英文词汇表长度→输入，目标语言中文词汇表长度→输出）\n",
    "INPUT_DIM = len(english_vocab)  # 源语言（英文）词汇表大小\n",
    "OUTPUT_DIM = len(chinese_vocab)  # 目标语言（中文）词汇表大小\n",
    "D_MODEL = 64\n",
    "NHEAD = 4\n",
    "NUM_ENCODER_LAYERS = 2\n",
    "NUM_DECODER_LAYERS = 2\n",
    "DIM_FEEDFORWARD = 64\n",
    "DROPOUT = 0.05\n",
    "MAX_EPOCH = 200\n",
    "\n",
    "model = Transformer(INPUT_DIM, OUTPUT_DIM, D_MODEL, NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, DIM_FEEDFORWARD, DROPOUT)\n",
    "# print(model)\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    for i, (src, trg) in enumerate(dataloader):\n",
    "        trg_mask = nn.Transformer.generate_square_subsequent_mask(trg.size(0)-1).bool()\n",
    "        padding_mask = (trg[1:,] == 0).transpose(0, 1)\n",
    "        output = model(src, trg[:-1,], trg_mask=trg_mask, padding_mask=padding_mask)\n",
    "        loss = criterion(output.view(-1, OUTPUT_DIM), trg[1:,].view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 99:     \n",
    "        print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training Finished\")\n",
    "\n",
    "\n",
    "\n",
    "# 保存模型\n",
    "model_save_path = \"./model/mymodel_en2zh.pth\"\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model have saved to {model_save_path}\")\n",
    "\n",
    "\n",
    "# 翻译函数\n",
    "def translate_sentence(sentence, src_vocab, trg_vocab, model, max_len=50):\n",
    "    model.eval()\n",
    "    # 更改分词函数\n",
    "    tokens = tokenize_en(sentence)\n",
    "    indices = sentence_to_indices(tokens, src_vocab)    \n",
    "    src_tensor = torch.tensor(indices).unsqueeze(1)\n",
    "    #src_len=torch.tensor(len(indices)).unsqueeze(0)\n",
    "    #print(\"src_tensor:\",src_tensor)\n",
    "    trg_indices = [trg_vocab['<sos>']]  # 目标语言以<sos>开始\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.tensor(trg_indices).unsqueeze(1)\n",
    "        with torch.no_grad():\n",
    "            output = model(src_tensor, trg_tensor)\n",
    "        pred_token = output.argmax(2)[-1].item()\n",
    "        trg_indices.append(pred_token)\n",
    "        if pred_token == trg_vocab['<eos>']:\n",
    "            break\n",
    "    \n",
    "    # 转换为目标语言（中文）tokens\n",
    "    trg_tokens = [list(trg_vocab.keys())[list(trg_vocab.values()).index(i)] for i in trg_indices]\n",
    "    final_tokens = [token for token in trg_tokens if token not in ['<sos>', '<eos>']]\n",
    "    return ' '.join(final_tokens)\n",
    "\n",
    "# 加载模型（结构不变）\n",
    "loaded_model = Transformer(INPUT_DIM, OUTPUT_DIM, D_MODEL, NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, DIM_FEEDFORWARD, DROPOUT)\n",
    "loaded_model.load_state_dict(torch.load(model_save_path, weights_only=True))\n",
    "\n",
    "# 测试翻译（输入英文句子）\n",
    "english_test_sentences = [\"Hello\", \"today weather very good\", \"I love dog\", \"I like cat\",\"love cat\"]\n",
    "for sentence in english_test_sentences:\n",
    "    translation = translate_sentence(sentence, english_vocab, chinese_vocab, loaded_model)\n",
    "    print(f'Input: {sentence} -> Translated: {translation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7780fd",
   "metadata": {},
   "source": [
    "Loss明显降低，且正确翻译了“I like cat”，看来增加维度和多头数量可以使模型更正确的定位每个词在向量空间中的位置，从而更准确的翻译\n",
    "以下再尝试不同的激活函数（GELU）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58024758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['你好', '今天 天气 很 好', '今天 天气 很 好', '我 爱 学习', '我 喜欢 狗', '天气 很 好', '我 爱 养猫', '我 喜欢 学习', '你好', '今天 天气 很 好', '爱 养猫', '今天', '天气', '很', '好', '我', '爱', '学习', '我', '喜欢', '狗', '猫']\n",
      "['Hello', 'today weather very good', 'today weather very good', 'I love learning', 'I like dog', 'weather very good', 'I love cat', 'I like study', 'Hello', 'today weather very good', 'love cat', 'today', 'weather', 'very', 'good', 'I', 'love', 'learning', 'I', 'like', 'dog', 'cat']\n",
      "{'你好': 4, '今天': 5, '天气': 6, '很': 7, '好': 8, '我': 9, '爱': 10, '学习': 11, '喜欢': 12, '狗': 13, '养': 14, '猫': 15, '<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
      "{'Hello': 4, 'today': 5, 'weather': 6, 'very': 7, 'good': 8, 'I': 9, 'love': 10, 'learning': 11, 'like': 12, 'dog': 13, 'cat': 14, 'study': 15, '<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
      "[([1, 4, 2], [1, 4, 2]), ([1, 5, 6, 7, 8, 2], [1, 5, 6, 7, 8, 2]), ([1, 5, 6, 7, 8, 2], [1, 5, 6, 7, 8, 2]), ([1, 9, 10, 11, 2], [1, 9, 10, 11, 2]), ([1, 9, 12, 13, 2], [1, 9, 12, 13, 2]), ([1, 6, 7, 8, 2], [1, 6, 7, 8, 2]), ([1, 9, 10, 14, 2], [1, 9, 10, 14, 15, 2]), ([1, 9, 12, 15, 2], [1, 9, 12, 11, 2]), ([1, 4, 2], [1, 4, 2]), ([1, 5, 6, 7, 8, 2], [1, 5, 6, 7, 8, 2]), ([1, 10, 14, 2], [1, 10, 14, 15, 2]), ([1, 5, 2], [1, 5, 2]), ([1, 6, 2], [1, 6, 2]), ([1, 7, 2], [1, 7, 2]), ([1, 8, 2], [1, 8, 2]), ([1, 9, 2], [1, 9, 2]), ([1, 10, 2], [1, 10, 2]), ([1, 11, 2], [1, 11, 2]), ([1, 9, 2], [1, 9, 2]), ([1, 12, 2], [1, 12, 2]), ([1, 13, 2], [1, 13, 2]), ([1, 14, 2], [1, 15, 2])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python 3.11.5\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 0.04443631321191788\n",
      "Epoch 200, Loss: 0.0012153564020991325\n",
      "Training Finished\n",
      "Model have saved to ./model/mymodel_en2zh.pth\n",
      "Input: Hello -> Translated: 你好\n",
      "Input: today weather very good -> Translated: 今天 天气 很 好\n",
      "Input: I love dog -> Translated: 我 爱 狗\n",
      "Input: I like cat -> Translated: 我 喜欢 学习\n",
      "Input: love cat -> Translated: 爱 养 猫\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import spacy\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# 设置随机种子以确保可重复性\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 中文和英文句子\n",
    "chinese_sentences = [\n",
    "    \"你好\", \"今天 天气 很 好\",\n",
    "    \"今天 天气 很 好\",\n",
    "    \"我 爱 学习\", \"我 喜欢 狗\",\n",
    "    \"天气 很 好\", \"我 爱 养猫\", \"我 喜欢 学习\",\n",
    "    \"你好\", \"今天 天气 很 好\", \"爱 养猫\",\n",
    "    \"今天\", \"天气\", \"很\", \"好\",\n",
    "    \"我\", \"爱\", \"学习\", \"我\", \"喜欢\", \"狗\", \"猫\",\n",
    "]\n",
    "english_sentences = [\n",
    "    \"Hello\", \"today weather very good\",\n",
    "    \"today weather very good\",\n",
    "    \"I love learning\", \"I like dog\",\n",
    "    \"weather very good\", \"I love cat\", \"I like study\",\n",
    "    \"Hello\", \"today weather very good\", \"love cat\",\n",
    "    \"today\", \"weather\", \"very\", \"good\",\n",
    "    \"I\", \"love\", \"learning\", \"I\", \"like\", \"dog\", \"cat\",\n",
    "]\n",
    "\n",
    "print(chinese_sentences)\n",
    "print(english_sentences)\n",
    "\n",
    "\n",
    "# 加载 spacy 分词器\n",
    "spacy_ch = spacy.load('zh_core_web_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "# 分词函数\n",
    "def tokenize_ch(text):\n",
    "    return [tok.text for tok in spacy_ch.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# 构建词汇表\n",
    "\n",
    "def build_vocab(data, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for tokens in data:\n",
    "        counter.update(tokens)\n",
    "    vocab = {word: idx + 4 for idx, (word, freq) in enumerate(counter.items()) if freq >= min_freq}\n",
    "    vocab['<pad>'] = 0\n",
    "    vocab['<sos>'] = 1\n",
    "    vocab['<eos>'] = 2\n",
    "    vocab['<unk>'] = 3\n",
    "    return vocab\n",
    "\n",
    "# 构建中文和英文词汇表\n",
    "chinese_vocab = build_vocab([tokenize_ch(s) for s in chinese_sentences])\n",
    "english_vocab = build_vocab([tokenize_en(s) for s in english_sentences])\n",
    "\n",
    "def sentence_to_indices(sentence, vocab):\n",
    "    return [vocab['<sos>']] + [vocab.get(word, vocab['<unk>']) for word in sentence] + [vocab['<eos>']]\n",
    "\n",
    "# 将句子转换为索引序列\n",
    "data = [\n",
    "    (sentence_to_indices(tokenize_en(english), english_vocab),  # 英文句子和英文词汇表\n",
    "     sentence_to_indices(tokenize_ch(chinese), chinese_vocab)  # 中文句子和中文词汇表\n",
    "     )\n",
    "    for chinese, english in zip(chinese_sentences, english_sentences)\n",
    "]\n",
    "print(chinese_vocab)\n",
    "print(english_vocab)\n",
    "print(data)\n",
    "\n",
    "\n",
    "# 数据整理函数\n",
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    src_pad = pad_sequence([torch.tensor(s) for s in src_batch], padding_value=0, batch_first=False)\n",
    "    trg_pad = pad_sequence([torch.tensor(t) for t in trg_batch], padding_value=0, batch_first=False)\n",
    "    return src_pad, trg_pad\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "dataset = TranslationDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Transformer模型（修改输入输出维度的含义，代码结构不变）\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward,\n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward,\n",
    "                                          dropout, batch_first=False)\n",
    "        self.fc1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.activation = nn.GELU()\n",
    "        self.fc2 = nn.Linear(dim_feedforward, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _generate_positional_encoding(self, seq_len):\n",
    "        position = torch.arange(seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2, dtype=torch.float) * (-math.log(10000.0) / self.d_model))\n",
    "        pe = torch.zeros(seq_len, self.d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(1)  # [1, seq_len, d_model]\n",
    "\n",
    "    def forward(self, src, trg, trg_mask=None, padding_mask=None):\n",
    "        src_seq_length, N = src.shape\n",
    "        trg_seq_length, N = trg.shape\n",
    "        # 动态生成位置编码\n",
    "        src_pos = self._generate_positional_encoding(src_seq_length).to(src.device)\n",
    "        trg_pos = self._generate_positional_encoding(trg_seq_length).to(trg.device)\n",
    "        # 扩展位置编码的形状以匹配输入\n",
    "        src_pos = src_pos.expand(-1, N, -1)  # [1, seq_len, d_model] -> [seq_len, batch_size, d_model]\n",
    "        trg_pos = trg_pos.expand(-1, N, -1)  # [1, seq_len, d_model] -> [seq_len, batch_size, d_model]\n",
    "\n",
    "        src = self.dropout(self.embedding(src) + src_pos)\n",
    "        trg = self.dropout(self.embedding(trg) + trg_pos)\n",
    "        if trg_mask is None:\n",
    "            output = self.transformer(src, trg)\n",
    "        else:\n",
    "            output = self.transformer(src, trg, tgt_mask=trg_mask, tgt_key_padding_mask=padding_mask)\n",
    "        output = self.fc1(output)\n",
    "        output = self.activation(output)\n",
    "        prediction = self.fc2(output)\n",
    "        return prediction\n",
    "\n",
    "\n",
    "# 调整输入输出维度（源语言英文词汇表长度→输入，目标语言中文词汇表长度→输出）\n",
    "INPUT_DIM = len(english_vocab)  # 源语言（英文）词汇表大小\n",
    "OUTPUT_DIM = len(chinese_vocab)  # 目标语言（中文）词汇表大小\n",
    "D_MODEL = 64\n",
    "NHEAD = 4\n",
    "NUM_ENCODER_LAYERS = 2\n",
    "NUM_DECODER_LAYERS = 2\n",
    "DIM_FEEDFORWARD = 64\n",
    "DROPOUT = 0.05\n",
    "MAX_EPOCH = 200\n",
    "\n",
    "model = Transformer(INPUT_DIM, OUTPUT_DIM, D_MODEL, NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, DIM_FEEDFORWARD,\n",
    "                    DROPOUT)\n",
    "# print(model)\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    for i, (src, trg) in enumerate(dataloader):\n",
    "        trg_mask = nn.Transformer.generate_square_subsequent_mask(trg.size(0) - 1).bool()\n",
    "        padding_mask = (trg[1:,] == 0).transpose(0, 1)\n",
    "        output = model(src, trg[:-1,], trg_mask=trg_mask, padding_mask=padding_mask)\n",
    "        loss = criterion(output.view(-1, OUTPUT_DIM), trg[1:,].view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 99:\n",
    "        print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training Finished\")\n",
    "\n",
    "# 保存模型\n",
    "model_save_path = \"./model/mymodel_en2zh.pth\"\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model have saved to {model_save_path}\")\n",
    "\n",
    "\n",
    "# 翻译函数\n",
    "def translate_sentence(sentence, src_vocab, trg_vocab, model, max_len=50):\n",
    "    model.eval()\n",
    "    # 更改分词函数\n",
    "    tokens = tokenize_en(sentence)\n",
    "    indices = sentence_to_indices(tokens, src_vocab)\n",
    "    src_tensor = torch.tensor(indices).unsqueeze(1)\n",
    "    # src_len=torch.tensor(len(indices)).unsqueeze(0)\n",
    "    # print(\"src_tensor:\",src_tensor)\n",
    "    trg_indices = [trg_vocab['<sos>']]  # 目标语言以<sos>开始\n",
    "\n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.tensor(trg_indices).unsqueeze(1)\n",
    "        with torch.no_grad():\n",
    "            output = model(src_tensor, trg_tensor)\n",
    "        pred_token = output.argmax(2)[-1].item()\n",
    "        trg_indices.append(pred_token)\n",
    "        if pred_token == trg_vocab['<eos>']:\n",
    "            break\n",
    "\n",
    "    # 转换为目标语言（中文）tokens\n",
    "    trg_tokens = [list(trg_vocab.keys())[list(trg_vocab.values()).index(i)] for i in trg_indices]\n",
    "    final_tokens = [token for token in trg_tokens if token not in ['<sos>', '<eos>']]\n",
    "    return ' '.join(final_tokens)\n",
    "\n",
    "# 加载模型（结构不变）\n",
    "loaded_model = Transformer(INPUT_DIM, OUTPUT_DIM, D_MODEL, NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, DIM_FEEDFORWARD,\n",
    "                           DROPOUT)\n",
    "loaded_model.load_state_dict(torch.load(model_save_path, weights_only=True))\n",
    "\n",
    "# 测试翻译（输入英文句子）\n",
    "english_test_sentences = [\"Hello\", \"today weather very good\", \"I love dog\", \"I like cat\", \"love cat\"]\n",
    "for sentence in english_test_sentences:\n",
    "    translation = translate_sentence(sentence, english_vocab, chinese_vocab, loaded_model)\n",
    "    print(f'Input: {sentence} -> Translated: {translation}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7826375a",
   "metadata": {},
   "source": [
    "还是ReLU激活函数效果好，下面尝试另一种损失函数（ nn.KLDivLoss 散度损失，这个损失函数可以用于衡量两个概率分布之间的差异，可能也适用于自然语言处理任务）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bad66966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['你好', '今天 天气 很 好', '今天 天气 很 好', '我 爱 学习', '我 喜欢 狗', '天气 很 好', '我 爱 养猫', '我 喜欢 学习', '你好', '今天 天气 很 好', '爱 养猫', '今天', '天气', '很', '好', '我', '爱', '学习', '我', '喜欢', '狗', '猫']\n",
      "['Hello', 'today weather very good', 'today weather very good', 'I love learning', 'I like dog', 'weather very good', 'I love cat', 'I like study', 'Hello', 'today weather very good', 'love cat', 'today', 'weather', 'very', 'good', 'I', 'love', 'learning', 'I', 'like', 'dog', 'cat']\n",
      "{'你好': 4, '今天': 5, '天气': 6, '很': 7, '好': 8, '我': 9, '爱': 10, '学习': 11, '喜欢': 12, '狗': 13, '养': 14, '猫': 15, '<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
      "{'Hello': 4, 'today': 5, 'weather': 6, 'very': 7, 'good': 8, 'I': 9, 'love': 10, 'learning': 11, 'like': 12, 'dog': 13, 'cat': 14, 'study': 15, '<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
      "[([1, 4, 2], [1, 4, 2]), ([1, 5, 6, 7, 8, 2], [1, 5, 6, 7, 8, 2]), ([1, 5, 6, 7, 8, 2], [1, 5, 6, 7, 8, 2]), ([1, 9, 10, 11, 2], [1, 9, 10, 11, 2]), ([1, 9, 12, 13, 2], [1, 9, 12, 13, 2]), ([1, 6, 7, 8, 2], [1, 6, 7, 8, 2]), ([1, 9, 10, 14, 2], [1, 9, 10, 14, 15, 2]), ([1, 9, 12, 15, 2], [1, 9, 12, 11, 2]), ([1, 4, 2], [1, 4, 2]), ([1, 5, 6, 7, 8, 2], [1, 5, 6, 7, 8, 2]), ([1, 10, 14, 2], [1, 10, 14, 15, 2]), ([1, 5, 2], [1, 5, 2]), ([1, 6, 2], [1, 6, 2]), ([1, 7, 2], [1, 7, 2]), ([1, 8, 2], [1, 8, 2]), ([1, 9, 2], [1, 9, 2]), ([1, 10, 2], [1, 10, 2]), ([1, 11, 2], [1, 11, 2]), ([1, 9, 2], [1, 9, 2]), ([1, 12, 2], [1, 12, 2]), ([1, 13, 2], [1, 13, 2]), ([1, 14, 2], [1, 15, 2])]\n",
      "Epoch 100, Loss: 0.00631779246032238\n",
      "Epoch 200, Loss: 0.0035212922375649214\n",
      "Training Finished\n",
      "Model have saved to ./model/mymodel_en2zh.pth\n",
      "Input: Hello -> Translated: 你好\n",
      "Input: today weather very good -> Translated: 今天 天气 很 好\n",
      "Input: I love dog -> Translated: 我 爱 狗\n",
      "Input: I like cat -> Translated: 我 喜欢 养 猫\n",
      "Input: love cat -> Translated: 爱 养 猫\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import spacy\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# 设置随机种子以确保可重复性\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 中文和英文句子\n",
    "chinese_sentences = [\n",
    "    \"你好\", \"今天 天气 很 好\",\n",
    "    \"今天 天气 很 好\",\n",
    "    \"我 爱 学习\", \"我 喜欢 狗\",\n",
    "    \"天气 很 好\", \"我 爱 养猫\", \"我 喜欢 学习\",\n",
    "    \"你好\", \"今天 天气 很 好\", \"爱 养猫\",\n",
    "    \"今天\", \"天气\", \"很\", \"好\",\n",
    "    \"我\", \"爱\", \"学习\", \"我\", \"喜欢\", \"狗\", \"猫\",\n",
    "]\n",
    "english_sentences = [\n",
    "    \"Hello\", \"today weather very good\",\n",
    "    \"today weather very good\",\n",
    "    \"I love learning\", \"I like dog\",\n",
    "    \"weather very good\", \"I love cat\", \"I like study\",\n",
    "    \"Hello\", \"today weather very good\", \"love cat\",\n",
    "    \"today\", \"weather\", \"very\", \"good\",\n",
    "    \"I\", \"love\", \"learning\", \"I\", \"like\", \"dog\", \"cat\",\n",
    "]\n",
    "\n",
    "print(chinese_sentences)\n",
    "print(english_sentences)\n",
    "\n",
    "\n",
    "# 加载 spacy 分词器\n",
    "spacy_ch = spacy.load('zh_core_web_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "# 分词函数\n",
    "def tokenize_ch(text):\n",
    "    return [tok.text for tok in spacy_ch.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# 构建词汇表\n",
    "\n",
    "def build_vocab(data, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for tokens in data:\n",
    "        counter.update(tokens)\n",
    "    vocab = {word: idx + 4 for idx, (word, freq) in enumerate(counter.items()) if freq >= min_freq}\n",
    "    vocab['<pad>'] = 0\n",
    "    vocab['<sos>'] = 1\n",
    "    vocab['<eos>'] = 2\n",
    "    vocab['<unk>'] = 3\n",
    "    return vocab\n",
    "\n",
    "# 构建中文和英文词汇表\n",
    "chinese_vocab = build_vocab([tokenize_ch(s) for s in chinese_sentences])\n",
    "english_vocab = build_vocab([tokenize_en(s) for s in english_sentences])\n",
    "\n",
    "def sentence_to_indices(sentence, vocab):\n",
    "    return [vocab['<sos>']] + [vocab.get(word, vocab['<unk>']) for word in sentence] + [vocab['<eos>']]\n",
    "\n",
    "# 将句子转换为索引序列\n",
    "data = [\n",
    "    (sentence_to_indices(tokenize_en(english), english_vocab),  # 英文句子和英文词汇表\n",
    "     sentence_to_indices(tokenize_ch(chinese), chinese_vocab)  # 中文句子和中文词汇表\n",
    "     )\n",
    "    for chinese, english in zip(chinese_sentences, english_sentences)\n",
    "]\n",
    "print(chinese_vocab)\n",
    "print(english_vocab)\n",
    "print(data)\n",
    "\n",
    "\n",
    "# 数据整理函数\n",
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    src_pad = pad_sequence([torch.tensor(s) for s in src_batch], padding_value=0, batch_first=False)\n",
    "    trg_pad = pad_sequence([torch.tensor(t) for t in trg_batch], padding_value=0, batch_first=False)\n",
    "    return src_pad, trg_pad\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "dataset = TranslationDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Transformer模型（修改输入输出维度的含义，代码结构不变）\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward,\n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward,\n",
    "                                          dropout, batch_first=False)\n",
    "        self.fc_out = nn.Linear(d_model, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _generate_positional_encoding(self, seq_len):\n",
    "        position = torch.arange(seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2, dtype=torch.float) * (-math.log(10000.0) / self.d_model))\n",
    "        pe = torch.zeros(seq_len, self.d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(1)  # [1, seq_len, d_model]\n",
    "\n",
    "    def forward(self, src, trg, trg_mask=None, padding_mask=None):\n",
    "        src_seq_length, N = src.shape\n",
    "        trg_seq_length, N = trg.shape\n",
    "        # 动态生成位置编码\n",
    "        src_pos = self._generate_positional_encoding(src_seq_length).to(src.device)\n",
    "        trg_pos = self._generate_positional_encoding(trg_seq_length).to(trg.device)\n",
    "        # 扩展位置编码的形状以匹配输入\n",
    "        src_pos = src_pos.expand(-1, N, -1)  # [1, seq_len, d_model] -> [seq_len, batch_size, d_model]\n",
    "        trg_pos = trg_pos.expand(-1, N, -1)  # [1, seq_len, d_model] -> [seq_len, batch_size, d_model]\n",
    "\n",
    "        src = self.dropout(self.embedding(src) + src_pos)\n",
    "        trg = self.dropout(self.embedding(trg) + trg_pos)\n",
    "        if trg_mask is None:\n",
    "            output = self.transformer(src, trg)\n",
    "        else:\n",
    "            output = self.transformer(src, trg, tgt_mask=trg_mask, tgt_key_padding_mask=padding_mask)\n",
    "        prediction = self.fc_out(output)\n",
    "        return prediction\n",
    "\n",
    "\n",
    "# 调整输入输出维度（源语言英文词汇表长度→输入，目标语言中文词汇表长度→输出）\n",
    "INPUT_DIM = len(english_vocab)  # 源语言（英文）词汇表大小\n",
    "OUTPUT_DIM = len(chinese_vocab)  # 目标语言（中文）词汇表大小\n",
    "D_MODEL = 64\n",
    "NHEAD = 4\n",
    "NUM_ENCODER_LAYERS = 2\n",
    "NUM_DECODER_LAYERS = 2\n",
    "DIM_FEEDFORWARD = 64\n",
    "DROPOUT = 0.05\n",
    "MAX_EPOCH = 200\n",
    "\n",
    "model = Transformer(INPUT_DIM, OUTPUT_DIM, D_MODEL, NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, DIM_FEEDFORWARD,\n",
    "                    DROPOUT)\n",
    "# print(model)\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.KLDivLoss(reduction='batchmean', log_target=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    for i, (src, trg) in enumerate(dataloader):\n",
    "        trg_mask = nn.Transformer.generate_square_subsequent_mask(trg.size(0) - 1).bool()\n",
    "        padding_mask = (trg[1:,] == 0).transpose(0, 1)\n",
    "        output = model(src, trg[:-1,], trg_mask=trg_mask, padding_mask=padding_mask)\n",
    "        # 将模型输出转换为对数概率分布\n",
    "        output_log_probs = nn.functional.log_softmax(output.view(-1, OUTPUT_DIM), dim=1)\n",
    "        # 将目标转换为 one - hot 编码\n",
    "        trg_one_hot = torch.nn.functional.one_hot(trg[1:,].view(-1), num_classes=OUTPUT_DIM).float()\n",
    "        loss = criterion(output_log_probs, trg_one_hot)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 99:\n",
    "        print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training Finished\")\n",
    "\n",
    "# 保存模型\n",
    "model_save_path = \"./model/mymodel_en2zh.pth\"\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model have saved to {model_save_path}\")\n",
    "\n",
    "\n",
    "# 翻译函数\n",
    "def translate_sentence(sentence, src_vocab, trg_vocab, model, max_len=50):\n",
    "    model.eval()\n",
    "    # 更改分词函数\n",
    "    tokens = tokenize_en(sentence)\n",
    "    indices = sentence_to_indices(tokens, src_vocab)\n",
    "    src_tensor = torch.tensor(indices).unsqueeze(1)\n",
    "    # src_len=torch.tensor(len(indices)).unsqueeze(0)\n",
    "    # print(\"src_tensor:\",src_tensor)\n",
    "    trg_indices = [trg_vocab['<sos>']]  # 目标语言以<sos>开始\n",
    "\n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.tensor(trg_indices).unsqueeze(1)\n",
    "        with torch.no_grad():\n",
    "            output = model(src_tensor, trg_tensor)\n",
    "        pred_token = output.argmax(2)[-1].item()\n",
    "        trg_indices.append(pred_token)\n",
    "        if pred_token == trg_vocab['<eos>']:\n",
    "            break\n",
    "\n",
    "    # 转换为目标语言（中文）tokens\n",
    "    trg_tokens = [list(trg_vocab.keys())[list(trg_vocab.values()).index(i)] for i in trg_indices]\n",
    "    final_tokens = [token for token in trg_tokens if token not in ['<sos>', '<eos>']]\n",
    "    return ' '.join(final_tokens)\n",
    "\n",
    "# 加载模型（结构不变）\n",
    "loaded_model = Transformer(INPUT_DIM, OUTPUT_DIM, D_MODEL, NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, DIM_FEEDFORWARD,\n",
    "                           DROPOUT)\n",
    "loaded_model.load_state_dict(torch.load(model_save_path, weights_only=True))\n",
    "\n",
    "# 测试翻译（输入英文句子）\n",
    "english_test_sentences = [\"Hello\", \"today weather very good\", \"I love dog\", \"I like cat\", \"love cat\"]\n",
    "for sentence in english_test_sentences:\n",
    "    translation = translate_sentence(sentence, english_vocab, chinese_vocab, loaded_model)\n",
    "    print(f'Input: {sentence} -> Translated: {translation}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7455309d",
   "metadata": {},
   "source": [
    "和CrossEntropyLoss的效果差不多，但收敛速度可以更快。以下再尝试`optim.SGD(model.parameters(), lr=0.005, momentum=0.9)`，可以在训练过程中根据损失函数的梯度更新模型参数，利用动量来加速收敛可能可以避免陷入局部最优解："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06668d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['你好', '今天 天气 很 好', '今天 天气 很 好', '我 爱 学习', '我 喜欢 狗', '天气 很 好', '我 爱 养猫', '我 喜欢 学习', '你好', '今天 天气 很 好', '爱 养猫', '今天', '天气', '很', '好', '我', '爱', '学习', '我', '喜欢', '狗', '猫']\n",
      "['Hello', 'today weather very good', 'today weather very good', 'I love learning', 'I like dog', 'weather very good', 'I love cat', 'I like study', 'Hello', 'today weather very good', 'love cat', 'today', 'weather', 'very', 'good', 'I', 'love', 'learning', 'I', 'like', 'dog', 'cat']\n",
      "{'你好': 4, '今天': 5, '天气': 6, '很': 7, '好': 8, '我': 9, '爱': 10, '学习': 11, '喜欢': 12, '狗': 13, '养': 14, '猫': 15, '<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
      "{'Hello': 4, 'today': 5, 'weather': 6, 'very': 7, 'good': 8, 'I': 9, 'love': 10, 'learning': 11, 'like': 12, 'dog': 13, 'cat': 14, 'study': 15, '<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
      "[([1, 4, 2], [1, 4, 2]), ([1, 5, 6, 7, 8, 2], [1, 5, 6, 7, 8, 2]), ([1, 5, 6, 7, 8, 2], [1, 5, 6, 7, 8, 2]), ([1, 9, 10, 11, 2], [1, 9, 10, 11, 2]), ([1, 9, 12, 13, 2], [1, 9, 12, 13, 2]), ([1, 6, 7, 8, 2], [1, 6, 7, 8, 2]), ([1, 9, 10, 14, 2], [1, 9, 10, 14, 15, 2]), ([1, 9, 12, 15, 2], [1, 9, 12, 11, 2]), ([1, 4, 2], [1, 4, 2]), ([1, 5, 6, 7, 8, 2], [1, 5, 6, 7, 8, 2]), ([1, 10, 14, 2], [1, 10, 14, 15, 2]), ([1, 5, 2], [1, 5, 2]), ([1, 6, 2], [1, 6, 2]), ([1, 7, 2], [1, 7, 2]), ([1, 8, 2], [1, 8, 2]), ([1, 9, 2], [1, 9, 2]), ([1, 10, 2], [1, 10, 2]), ([1, 11, 2], [1, 11, 2]), ([1, 9, 2], [1, 9, 2]), ([1, 12, 2], [1, 12, 2]), ([1, 13, 2], [1, 13, 2]), ([1, 14, 2], [1, 15, 2])]\n",
      "Epoch 100, Loss: 0.017708925530314445\n",
      "Epoch 200, Loss: 0.019534729421138763\n",
      "Training Finished\n",
      "Model have saved to ./model/mymodel_en2zh.pth\n",
      "Input: Hello -> Translated: 你好\n",
      "Input: today weather very good -> Translated: 今天 天气 很 好\n",
      "Input: I love dog -> Translated: 我 爱 狗\n",
      "Input: I like cat -> Translated: 我 喜欢 学习\n",
      "Input: love cat -> Translated: 爱 养 猫\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import spacy\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# 设置随机种子以确保可重复性\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 中文和英文句子\n",
    "chinese_sentences = [\n",
    "    \"你好\", \"今天 天气 很 好\",\n",
    "    \"今天 天气 很 好\",\n",
    "    \"我 爱 学习\", \"我 喜欢 狗\",\n",
    "    \"天气 很 好\", \"我 爱 养猫\", \"我 喜欢 学习\",\n",
    "    \"你好\", \"今天 天气 很 好\", \"爱 养猫\",\n",
    "    \"今天\", \"天气\", \"很\", \"好\",\n",
    "    \"我\", \"爱\", \"学习\", \"我\", \"喜欢\", \"狗\", \"猫\",\n",
    "]\n",
    "english_sentences = [\n",
    "    \"Hello\", \"today weather very good\",\n",
    "    \"today weather very good\",\n",
    "    \"I love learning\", \"I like dog\",\n",
    "    \"weather very good\", \"I love cat\", \"I like study\",\n",
    "    \"Hello\", \"today weather very good\", \"love cat\",\n",
    "    \"today\", \"weather\", \"very\", \"good\",\n",
    "    \"I\", \"love\", \"learning\", \"I\", \"like\", \"dog\", \"cat\",\n",
    "]\n",
    "\n",
    "print(chinese_sentences)\n",
    "print(english_sentences)\n",
    "\n",
    "\n",
    "# 加载 spacy 分词器\n",
    "spacy_ch = spacy.load('zh_core_web_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "# 分词函数\n",
    "def tokenize_ch(text):\n",
    "    return [tok.text for tok in spacy_ch.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# 构建词汇表\n",
    "\n",
    "def build_vocab(data, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for tokens in data:\n",
    "        counter.update(tokens)\n",
    "    vocab = {word: idx + 4 for idx, (word, freq) in enumerate(counter.items()) if freq >= min_freq}\n",
    "    vocab['<pad>'] = 0\n",
    "    vocab['<sos>'] = 1\n",
    "    vocab['<eos>'] = 2\n",
    "    vocab['<unk>'] = 3\n",
    "    return vocab\n",
    "\n",
    "# 构建中文和英文词汇表\n",
    "chinese_vocab = build_vocab([tokenize_ch(s) for s in chinese_sentences])\n",
    "english_vocab = build_vocab([tokenize_en(s) for s in english_sentences])\n",
    "\n",
    "def sentence_to_indices(sentence, vocab):\n",
    "    return [vocab['<sos>']] + [vocab.get(word, vocab['<unk>']) for word in sentence] + [vocab['<eos>']]\n",
    "\n",
    "# 将句子转换为索引序列\n",
    "data = [\n",
    "    (sentence_to_indices(tokenize_en(english), english_vocab),  # 英文句子和英文词汇表\n",
    "     sentence_to_indices(tokenize_ch(chinese), chinese_vocab)  # 中文句子和中文词汇表\n",
    "     )\n",
    "    for chinese, english in zip(chinese_sentences, english_sentences)\n",
    "]\n",
    "print(chinese_vocab)\n",
    "print(english_vocab)\n",
    "print(data)\n",
    "\n",
    "\n",
    "# 数据整理函数\n",
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    src_pad = pad_sequence([torch.tensor(s) for s in src_batch], padding_value=0, batch_first=False)\n",
    "    trg_pad = pad_sequence([torch.tensor(t) for t in trg_batch], padding_value=0, batch_first=False)\n",
    "    return src_pad, trg_pad\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "dataset = TranslationDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Transformer模型（修改输入输出维度的含义，代码结构不变）\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward,\n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward,\n",
    "                                          dropout, batch_first=False)\n",
    "        self.fc_out = nn.Linear(d_model, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _generate_positional_encoding(self, seq_len):\n",
    "        position = torch.arange(seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2, dtype=torch.float) * (-math.log(10000.0) / self.d_model))\n",
    "        pe = torch.zeros(seq_len, self.d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(1)  # [1, seq_len, d_model]\n",
    "\n",
    "    def forward(self, src, trg, trg_mask=None, padding_mask=None):\n",
    "        src_seq_length, N = src.shape\n",
    "        trg_seq_length, N = trg.shape\n",
    "        # 动态生成位置编码\n",
    "        src_pos = self._generate_positional_encoding(src_seq_length).to(src.device)\n",
    "        trg_pos = self._generate_positional_encoding(trg_seq_length).to(trg.device)\n",
    "        # 扩展位置编码的形状以匹配输入\n",
    "        src_pos = src_pos.expand(-1, N, -1)  # [1, seq_len, d_model] -> [seq_len, batch_size, d_model]\n",
    "        trg_pos = trg_pos.expand(-1, N, -1)  # [1, seq_len, d_model] -> [seq_len, batch_size, d_model]\n",
    "\n",
    "        src = self.dropout(self.embedding(src) + src_pos)\n",
    "        trg = self.dropout(self.embedding(trg) + trg_pos)\n",
    "        if trg_mask is None:\n",
    "            output = self.transformer(src, trg)\n",
    "        else:\n",
    "            output = self.transformer(src, trg, tgt_mask=trg_mask, tgt_key_padding_mask=padding_mask)\n",
    "        prediction = self.fc_out(output)\n",
    "        return prediction\n",
    "\n",
    "\n",
    "# 调整输入输出维度（源语言英文词汇表长度→输入，目标语言中文词汇表长度→输出）\n",
    "INPUT_DIM = len(english_vocab)  # 源语言（英文）词汇表大小\n",
    "OUTPUT_DIM = len(chinese_vocab)  # 目标语言（中文）词汇表大小\n",
    "D_MODEL = 64\n",
    "NHEAD = 4\n",
    "NUM_ENCODER_LAYERS = 2\n",
    "NUM_DECODER_LAYERS = 2\n",
    "DIM_FEEDFORWARD = 64\n",
    "DROPOUT = 0.05\n",
    "MAX_EPOCH = 200\n",
    "\n",
    "model = Transformer(INPUT_DIM, OUTPUT_DIM, D_MODEL, NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, DIM_FEEDFORWARD,\n",
    "                    DROPOUT)\n",
    "# print(model)\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    for i, (src, trg) in enumerate(dataloader):\n",
    "        trg_mask = nn.Transformer.generate_square_subsequent_mask(trg.size(0) - 1).bool()\n",
    "        padding_mask = (trg[1:,] == 0).transpose(0, 1)\n",
    "        output = model(src, trg[:-1,], trg_mask=trg_mask, padding_mask=padding_mask)\n",
    "        loss = criterion(output.view(-1, OUTPUT_DIM), trg[1:,].view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 99:\n",
    "        print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training Finished\")\n",
    "\n",
    "# 保存模型\n",
    "model_save_path = \"./model/mymodel_en2zh.pth\"\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model have saved to {model_save_path}\")\n",
    "\n",
    "\n",
    "# 翻译函数\n",
    "def translate_sentence(sentence, src_vocab, trg_vocab, model, max_len=50):\n",
    "    model.eval()\n",
    "    # 更改分词函数\n",
    "    tokens = tokenize_en(sentence)\n",
    "    indices = sentence_to_indices(tokens, src_vocab)\n",
    "    src_tensor = torch.tensor(indices).unsqueeze(1)\n",
    "    # src_len=torch.tensor(len(indices)).unsqueeze(0)\n",
    "    # print(\"src_tensor:\",src_tensor)\n",
    "    trg_indices = [trg_vocab['<sos>']]  # 目标语言以<sos>开始\n",
    "\n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.tensor(trg_indices).unsqueeze(1)\n",
    "        with torch.no_grad():\n",
    "            output = model(src_tensor, trg_tensor)\n",
    "        pred_token = output.argmax(2)[-1].item()\n",
    "        trg_indices.append(pred_token)\n",
    "        if pred_token == trg_vocab['<eos>']:\n",
    "            break\n",
    "\n",
    "    # 转换为目标语言（中文）tokens\n",
    "    trg_tokens = [list(trg_vocab.keys())[list(trg_vocab.values()).index(i)] for i in trg_indices]\n",
    "    final_tokens = [token for token in trg_tokens if token not in ['<sos>', '<eos>']]\n",
    "    return ' '.join(final_tokens)\n",
    "\n",
    "# 加载模型（结构不变）\n",
    "loaded_model = Transformer(INPUT_DIM, OUTPUT_DIM, D_MODEL, NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, DIM_FEEDFORWARD,\n",
    "                           DROPOUT)\n",
    "loaded_model.load_state_dict(torch.load(model_save_path, weights_only=True))\n",
    "\n",
    "# 测试翻译（输入英文句子）\n",
    "english_test_sentences = [\"Hello\", \"today weather very good\", \"I love dog\", \"I like cat\", \"love cat\"]\n",
    "for sentence in english_test_sentences:\n",
    "    translation = translate_sentence(sentence, english_vocab, chinese_vocab, loaded_model)\n",
    "    print(f'Input: {sentence} -> Translated: {translation}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a64e31",
   "metadata": {},
   "source": [
    "实际效果并不是很好，还是使用原来的学习率策略，最后扫一下dropouts，batch_sizes，max_epochs等参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6462ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['你好', '今天 天气 很 好', '今天 天气 很 好', '我 爱 学习', '我 喜欢 狗', '天气 很 好', '我 爱 养猫', '我 喜欢 学习', '你好', '今天 天气 很 好', '爱 养猫', '今天', '天气', '很', '好', '我', '爱', '学习', '我', '喜欢', '狗', '猫']\n",
      "['Hello', 'today weather very good', 'today weather very good', 'I love learning', 'I like dog', 'weather very good', 'I love cat', 'I like study', 'Hello', 'today weather very good', 'love cat', 'today', 'weather', 'very', 'good', 'I', 'love', 'learning', 'I', 'like', 'dog', 'cat']\n",
      "{'你好': 4, '今天': 5, '天气': 6, '很': 7, '好': 8, '我': 9, '爱': 10, '学习': 11, '喜欢': 12, '狗': 13, '养': 14, '猫': 15, '<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
      "{'Hello': 4, 'today': 5, 'weather': 6, 'very': 7, 'good': 8, 'I': 9, 'love': 10, 'learning': 11, 'like': 12, 'dog': 13, 'cat': 14, 'study': 15, '<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
      "[([1, 4, 2], [1, 4, 2]), ([1, 5, 6, 7, 8, 2], [1, 5, 6, 7, 8, 2]), ([1, 5, 6, 7, 8, 2], [1, 5, 6, 7, 8, 2]), ([1, 9, 10, 11, 2], [1, 9, 10, 11, 2]), ([1, 9, 12, 13, 2], [1, 9, 12, 13, 2]), ([1, 6, 7, 8, 2], [1, 6, 7, 8, 2]), ([1, 9, 10, 14, 2], [1, 9, 10, 14, 15, 2]), ([1, 9, 12, 15, 2], [1, 9, 12, 11, 2]), ([1, 4, 2], [1, 4, 2]), ([1, 5, 6, 7, 8, 2], [1, 5, 6, 7, 8, 2]), ([1, 10, 14, 2], [1, 10, 14, 15, 2]), ([1, 5, 2], [1, 5, 2]), ([1, 6, 2], [1, 6, 2]), ([1, 7, 2], [1, 7, 2]), ([1, 8, 2], [1, 8, 2]), ([1, 9, 2], [1, 9, 2]), ([1, 10, 2], [1, 10, 2]), ([1, 11, 2], [1, 11, 2]), ([1, 9, 2], [1, 9, 2]), ([1, 12, 2], [1, 12, 2]), ([1, 13, 2], [1, 13, 2]), ([1, 14, 2], [1, 15, 2])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python 3.11.5\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 0.00631779246032238, LR: 0.001, Dropout: 0.05, Batch Size: 8, Max Epoch: 200\n",
      "Epoch 200, Loss: 0.0035212922375649214, LR: 0.001, Dropout: 0.05, Batch Size: 8, Max Epoch: 200\n",
      "Epoch 100, Loss: 0.009551836177706718, LR: 0.001, Dropout: 0.05, Batch Size: 8, Max Epoch: 300\n",
      "Epoch 200, Loss: 0.0023489058949053288, LR: 0.001, Dropout: 0.05, Batch Size: 8, Max Epoch: 300\n",
      "Epoch 300, Loss: 0.008600478991866112, LR: 0.001, Dropout: 0.05, Batch Size: 8, Max Epoch: 300\n",
      "Epoch 100, Loss: 0.03161943331360817, LR: 0.001, Dropout: 0.05, Batch Size: 16, Max Epoch: 200\n",
      "Epoch 200, Loss: 0.004653779324144125, LR: 0.001, Dropout: 0.05, Batch Size: 16, Max Epoch: 200\n",
      "Epoch 100, Loss: 0.01760675199329853, LR: 0.001, Dropout: 0.05, Batch Size: 16, Max Epoch: 300\n",
      "Epoch 200, Loss: 0.0051731388084590435, LR: 0.001, Dropout: 0.05, Batch Size: 16, Max Epoch: 300\n",
      "Epoch 300, Loss: 0.004482160322368145, LR: 0.001, Dropout: 0.05, Batch Size: 16, Max Epoch: 300\n",
      "Epoch 100, Loss: 0.014309111051261425, LR: 0.001, Dropout: 0.1, Batch Size: 8, Max Epoch: 200\n",
      "Epoch 200, Loss: 0.019383562728762627, LR: 0.001, Dropout: 0.1, Batch Size: 8, Max Epoch: 200\n",
      "Epoch 100, Loss: 0.060758981853723526, LR: 0.001, Dropout: 0.1, Batch Size: 8, Max Epoch: 300\n",
      "Epoch 200, Loss: 0.0029878271743655205, LR: 0.001, Dropout: 0.1, Batch Size: 8, Max Epoch: 300\n",
      "Epoch 300, Loss: 0.0035807082895189524, LR: 0.001, Dropout: 0.1, Batch Size: 8, Max Epoch: 300\n",
      "Epoch 100, Loss: 0.04227443039417267, LR: 0.001, Dropout: 0.1, Batch Size: 16, Max Epoch: 200\n",
      "Epoch 200, Loss: 0.00869838148355484, LR: 0.001, Dropout: 0.1, Batch Size: 16, Max Epoch: 200\n",
      "Epoch 100, Loss: 0.05752184987068176, LR: 0.001, Dropout: 0.1, Batch Size: 16, Max Epoch: 300\n",
      "Epoch 200, Loss: 0.007140506524592638, LR: 0.001, Dropout: 0.1, Batch Size: 16, Max Epoch: 300\n",
      "Epoch 300, Loss: 0.0050012399442493916, LR: 0.001, Dropout: 0.1, Batch Size: 16, Max Epoch: 300\n",
      "Epoch 100, Loss: 0.7721593379974365, LR: 0.01, Dropout: 0.05, Batch Size: 8, Max Epoch: 200\n",
      "Epoch 200, Loss: 0.48719069361686707, LR: 0.01, Dropout: 0.05, Batch Size: 8, Max Epoch: 200\n",
      "Epoch 100, Loss: 1.0033267736434937, LR: 0.01, Dropout: 0.05, Batch Size: 8, Max Epoch: 300\n",
      "Epoch 200, Loss: 0.6705548763275146, LR: 0.01, Dropout: 0.05, Batch Size: 8, Max Epoch: 300\n",
      "Epoch 300, Loss: 0.6873266696929932, LR: 0.01, Dropout: 0.05, Batch Size: 8, Max Epoch: 300\n",
      "Epoch 100, Loss: 0.9492952227592468, LR: 0.01, Dropout: 0.05, Batch Size: 16, Max Epoch: 200\n",
      "Epoch 200, Loss: 1.0455340147018433, LR: 0.01, Dropout: 0.05, Batch Size: 16, Max Epoch: 200\n",
      "Epoch 100, Loss: 0.4347855746746063, LR: 0.01, Dropout: 0.05, Batch Size: 16, Max Epoch: 300\n",
      "Epoch 200, Loss: 0.4118514358997345, LR: 0.01, Dropout: 0.05, Batch Size: 16, Max Epoch: 300\n",
      "Epoch 300, Loss: 0.26741963624954224, LR: 0.01, Dropout: 0.05, Batch Size: 16, Max Epoch: 300\n",
      "Epoch 100, Loss: 2.125976800918579, LR: 0.01, Dropout: 0.1, Batch Size: 8, Max Epoch: 200\n",
      "Epoch 200, Loss: 2.547013521194458, LR: 0.01, Dropout: 0.1, Batch Size: 8, Max Epoch: 200\n",
      "Epoch 100, Loss: 0.48094743490219116, LR: 0.01, Dropout: 0.1, Batch Size: 8, Max Epoch: 300\n",
      "Epoch 200, Loss: 0.683197021484375, LR: 0.01, Dropout: 0.1, Batch Size: 8, Max Epoch: 300\n",
      "Epoch 300, Loss: 0.9152164459228516, LR: 0.01, Dropout: 0.1, Batch Size: 8, Max Epoch: 300\n",
      "Epoch 100, Loss: 0.6044336557388306, LR: 0.01, Dropout: 0.1, Batch Size: 16, Max Epoch: 200\n",
      "Epoch 200, Loss: 0.5350189805030823, LR: 0.01, Dropout: 0.1, Batch Size: 16, Max Epoch: 200\n",
      "Epoch 100, Loss: 0.975596010684967, LR: 0.01, Dropout: 0.1, Batch Size: 16, Max Epoch: 300\n",
      "Epoch 200, Loss: 0.9023725986480713, LR: 0.01, Dropout: 0.1, Batch Size: 16, Max Epoch: 300\n",
      "Epoch 300, Loss: 1.0640509128570557, LR: 0.01, Dropout: 0.1, Batch Size: 16, Max Epoch: 300\n",
      "Best learning rate: 0.001, Best dropout: 0.05, Best batch size: 8, Best max epoch: 300, Best loss: 0.09353994218153983\n",
      "Epoch 100, Loss: 0.009731768630445004\n",
      "Epoch 200, Loss: 0.009251739829778671\n",
      "Epoch 300, Loss: 0.004006659146398306\n",
      "Training Finished\n",
      "Model have saved to ./model/mymodel_en2zh.pth\n",
      "Input: Hello -> Translated: 你好\n",
      "Input: today weather very good -> Translated: 今天 天气 很 好\n",
      "Input: I love dog -> Translated: 我 爱 养 猫\n",
      "Input: I like cat -> Translated: 我 喜欢 养 猫\n",
      "Input: love cat -> Translated: 爱 养 猫\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import spacy\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# 设置随机种子以确保可重复性\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 中文和英文句子\n",
    "chinese_sentences = [\n",
    "    \"你好\", \"今天 天气 很 好\",\n",
    "    \"今天 天气 很 好\",\n",
    "    \"我 爱 学习\", \"我 喜欢 狗\",\n",
    "    \"天气 很 好\", \"我 爱 养猫\", \"我 喜欢 学习\",\n",
    "    \"你好\", \"今天 天气 很 好\", \"爱 养猫\",\n",
    "    \"今天\", \"天气\", \"很\", \"好\",\n",
    "    \"我\", \"爱\", \"学习\", \"我\", \"喜欢\", \"狗\", \"猫\",\n",
    "]\n",
    "english_sentences = [\n",
    "    \"Hello\", \"today weather very good\",\n",
    "    \"today weather very good\",\n",
    "    \"I love learning\", \"I like dog\",\n",
    "    \"weather very good\", \"I love cat\", \"I like study\",\n",
    "    \"Hello\", \"today weather very good\", \"love cat\",\n",
    "    \"today\", \"weather\", \"very\", \"good\",\n",
    "    \"I\", \"love\", \"learning\", \"I\", \"like\", \"dog\", \"cat\",\n",
    "]\n",
    "\n",
    "print(chinese_sentences)\n",
    "print(english_sentences)\n",
    "\n",
    "\n",
    "# 加载 spacy 分词器\n",
    "spacy_ch = spacy.load('zh_core_web_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "# 分词函数\n",
    "def tokenize_ch(text):\n",
    "    return [tok.text for tok in spacy_ch.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# 构建词汇表\n",
    "\n",
    "def build_vocab(data, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for tokens in data:\n",
    "        counter.update(tokens)\n",
    "    vocab = {word: idx + 4 for idx, (word, freq) in enumerate(counter.items()) if freq >= min_freq}\n",
    "    vocab['<pad>'] = 0\n",
    "    vocab['<sos>'] = 1\n",
    "    vocab['<eos>'] = 2\n",
    "    vocab['<unk>'] = 3\n",
    "    return vocab\n",
    "\n",
    "# 构建中文和英文词汇表\n",
    "chinese_vocab = build_vocab([tokenize_ch(s) for s in chinese_sentences])\n",
    "english_vocab = build_vocab([tokenize_en(s) for s in english_sentences])\n",
    "\n",
    "def sentence_to_indices(sentence, vocab):\n",
    "    return [vocab['<sos>']] + [vocab.get(word, vocab['<unk>']) for word in sentence] + [vocab['<eos>']]\n",
    "\n",
    "# 将句子转换为索引序列\n",
    "data = [\n",
    "    (sentence_to_indices(tokenize_en(english), english_vocab),  # 英文句子和英文词汇表\n",
    "     sentence_to_indices(tokenize_ch(chinese), chinese_vocab)  # 中文句子和中文词汇表\n",
    "     )\n",
    "    for chinese, english in zip(chinese_sentences, english_sentences)\n",
    "]\n",
    "print(chinese_vocab)\n",
    "print(english_vocab)\n",
    "print(data)\n",
    "\n",
    "\n",
    "# 数据整理函数\n",
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    src_pad = pad_sequence([torch.tensor(s) for s in src_batch], padding_value=0, batch_first=False)\n",
    "    trg_pad = pad_sequence([torch.tensor(t) for t in trg_batch], padding_value=0, batch_first=False)\n",
    "    return src_pad, trg_pad\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "# Transformer模型（修改输入输出维度的含义，代码结构不变）\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward,\n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward,\n",
    "                                          dropout, batch_first=False)\n",
    "        self.fc_out = nn.Linear(d_model, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _generate_positional_encoding(self, seq_len):\n",
    "        position = torch.arange(seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2, dtype=torch.float) * (-math.log(10000.0) / self.d_model))\n",
    "        pe = torch.zeros(seq_len, self.d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(1)  # [1, seq_len, d_model]\n",
    "\n",
    "    def forward(self, src, trg, trg_mask=None, padding_mask=None):\n",
    "        src_seq_length, N = src.shape\n",
    "        trg_seq_length, N = trg.shape\n",
    "        # 动态生成位置编码\n",
    "        src_pos = self._generate_positional_encoding(src_seq_length).to(src.device)\n",
    "        trg_pos = self._generate_positional_encoding(trg_seq_length).to(trg.device)\n",
    "        # 扩展位置编码的形状以匹配输入\n",
    "        src_pos = src_pos.expand(-1, N, -1)  # [1, seq_len, d_model] -> [seq_len, batch_size, d_model]\n",
    "        trg_pos = trg_pos.expand(-1, N, -1)  # [1, seq_len, d_model] -> [seq_len, batch_size, d_model]\n",
    "\n",
    "        src = self.dropout(self.embedding(src) + src_pos)\n",
    "        trg = self.dropout(self.embedding(trg) + trg_pos)\n",
    "        if trg_mask is None:\n",
    "            output = self.transformer(src, trg)\n",
    "        else:\n",
    "            output = self.transformer(src, trg, tgt_mask=trg_mask, tgt_key_padding_mask=padding_mask)\n",
    "        prediction = self.fc_out(output)\n",
    "        return prediction\n",
    "\n",
    "\n",
    "# 调整输入输出维度（源语言英文词汇表长度→输入，目标语言中文词汇表长度→输出）\n",
    "INPUT_DIM = len(english_vocab)  # 源语言（英文）词汇表大小\n",
    "OUTPUT_DIM = len(chinese_vocab)  # 目标语言（中文）词汇表大小\n",
    "D_MODEL = 64\n",
    "NHEAD = 4\n",
    "NUM_ENCODER_LAYERS = 2\n",
    "NUM_DECODER_LAYERS = 2\n",
    "DIM_FEEDFORWARD = 64\n",
    "\n",
    "# 定义要搜索的参数网格\n",
    "learning_rates = [0.001, 0.01]\n",
    "dropouts = [0.05, 0.1]\n",
    "batch_sizes = [8, 16]\n",
    "max_epochs = [200, 300]\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_lr = None\n",
    "best_dropout = None\n",
    "best_batch_size = None\n",
    "best_max_epoch = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for dropout in dropouts:\n",
    "        for batch_size in batch_sizes:\n",
    "            for max_epoch in max_epochs:\n",
    "                dataset = TranslationDataset(data)\n",
    "                dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "                model = Transformer(INPUT_DIM, OUTPUT_DIM, D_MODEL, NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS,\n",
    "                                    DIM_FEEDFORWARD, dropout)\n",
    "\n",
    "                # 定义损失函数和优化器\n",
    "                criterion = nn.KLDivLoss(reduction='batchmean', log_target=False)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                total_loss = 0\n",
    "                # 训练循环\n",
    "                for epoch in range(max_epoch):\n",
    "                    for i, (src, trg) in enumerate(dataloader):\n",
    "                        trg_mask = nn.Transformer.generate_square_subsequent_mask(trg.size(0) - 1).bool()\n",
    "                        padding_mask = (trg[1:,] == 0).transpose(0, 1)\n",
    "                        output = model(src, trg[:-1,], trg_mask=trg_mask, padding_mask=padding_mask)\n",
    "                        # 将模型输出转换为对数概率分布\n",
    "                        output_log_probs = nn.functional.log_softmax(output.view(-1, OUTPUT_DIM), dim=1)\n",
    "                        # 将目标转换为 one - hot 编码\n",
    "                        trg_one_hot = torch.nn.functional.one_hot(trg[1:,].view(-1), num_classes=OUTPUT_DIM).float()\n",
    "                        loss = criterion(output_log_probs, trg_one_hot)\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        total_loss += loss.item()\n",
    "\n",
    "                    if epoch % 100 == 99:\n",
    "                        print(f'Epoch {epoch + 1}, Loss: {loss.item()}, LR: {lr}, Dropout: {dropout}, Batch Size: {batch_size}, Max Epoch: {max_epoch}')\n",
    "\n",
    "                avg_loss = total_loss / (max_epoch * len(dataloader))\n",
    "                if avg_loss < best_loss:\n",
    "                    best_loss = avg_loss\n",
    "                    best_lr = lr\n",
    "                    best_dropout = dropout\n",
    "                    best_batch_size = batch_size\n",
    "                    best_max_epoch = max_epoch\n",
    "\n",
    "print(f\"Best learning rate: {best_lr}, Best dropout: {best_dropout}, Best batch size: {best_batch_size}, Best max epoch: {best_max_epoch}, Best loss: {best_loss}\")\n",
    "\n",
    "# 使用最佳参数重新训练模型\n",
    "dataset = TranslationDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=best_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model = Transformer(INPUT_DIM, OUTPUT_DIM, D_MODEL, NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS,\n",
    "                    DIM_FEEDFORWARD, best_dropout)\n",
    "criterion = nn.KLDivLoss(reduction='batchmean', log_target=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_lr)\n",
    "\n",
    "for epoch in range(best_max_epoch):\n",
    "    for i, (src, trg) in enumerate(dataloader):\n",
    "        trg_mask = nn.Transformer.generate_square_subsequent_mask(trg.size(0) - 1).bool()\n",
    "        padding_mask = (trg[1:,] == 0).transpose(0, 1)\n",
    "        output = model(src, trg[:-1,], trg_mask=trg_mask, padding_mask=padding_mask)\n",
    "        # 将模型输出转换为对数概率分布\n",
    "        output_log_probs = nn.functional.log_softmax(output.view(-1, OUTPUT_DIM), dim=1)\n",
    "        # 将目标转换为 one - hot 编码\n",
    "        trg_one_hot = torch.nn.functional.one_hot(trg[1:,].view(-1), num_classes=OUTPUT_DIM).float()\n",
    "        loss = criterion(output_log_probs, trg_one_hot)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 99:\n",
    "        print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training Finished\")\n",
    "\n",
    "# 保存模型\n",
    "model_save_path = \"./model/mymodel_en2zh.pth\"\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model have saved to {model_save_path}\")\n",
    "\n",
    "\n",
    "# 翻译函数\n",
    "def translate_sentence(sentence, src_vocab, trg_vocab, model, max_len=50):\n",
    "    model.eval()\n",
    "    # 更改分词函数\n",
    "    tokens = tokenize_en(sentence)\n",
    "    indices = sentence_to_indices(tokens, src_vocab)\n",
    "    src_tensor = torch.tensor(indices).unsqueeze(1)\n",
    "    # src_len=torch.tensor(len(indices)).unsqueeze(0)\n",
    "    # print(\"src_tensor:\",src_tensor)\n",
    "    trg_indices = [trg_vocab['<sos>']]  # 目标语言以<sos>开始\n",
    "\n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.tensor(trg_indices).unsqueeze(1)\n",
    "        with torch.no_grad():\n",
    "            output = model(src_tensor, trg_tensor)\n",
    "        pred_token = output.argmax(2)[-1].item()\n",
    "        trg_indices.append(pred_token)\n",
    "        if pred_token == trg_vocab['<eos>']:\n",
    "            break\n",
    "\n",
    "    # 转换为目标语言（中文）tokens\n",
    "    trg_tokens = [list(trg_vocab.keys())[list(trg_vocab.values()).index(i)] for i in trg_indices]\n",
    "    final_tokens = [token for token in trg_tokens if token not in ['<sos>', '<eos>']]\n",
    "    return ' '.join(final_tokens)\n",
    "\n",
    "# 加载模型（结构不变）\n",
    "loaded_model = Transformer(INPUT_DIM, OUTPUT_DIM, D_MODEL, NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, DIM_FEEDFORWARD,\n",
    "                           best_dropout)\n",
    "loaded_model.load_state_dict(torch.load(model_save_path, weights_only=True))\n",
    "\n",
    "# 测试翻译（输入英文句子）\n",
    "english_test_sentences = [\"Hello\", \"today weather very good\", \"I love dog\", \"I like cat\", \"love cat\"]\n",
    "for sentence in english_test_sentences:\n",
    "    translation = translate_sentence(sentence, english_vocab, chinese_vocab, loaded_model)\n",
    "    print(f'Input: {sentence} -> Translated: {translation}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c6bc45",
   "metadata": {},
   "source": [
    "最佳参数是Best learning rate: 0.001, Best dropout: 0.05, Best batch size: 8, Best max epoch: 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b46eebe",
   "metadata": {},
   "source": [
    "以下为完整的最终模型，具体修改为： \n",
    "\n",
    "D_MODEL：模型的隐藏层维度，值为 64    \n",
    "NHEAD：多头注意力机制的头数，值为 4    \n",
    "NUM_ENCODER_LAYERS：编码器的层数，值为 2 （不变）   \n",
    "NUM_DECODER_LAYERS：解码器的层数，值为 2  （不变）  \n",
    "DIM_FEEDFORWARD：前馈神经网络的隐藏层维度，值为 64    \n",
    "DROPOUT：Dropout 概率，值为 0.05   （不变）    \n",
    "MAX_EPOCH：训练轮次，值为 300   \n",
    "batch_size：批大小，在 DataLoader 里设置为 8   （不变）\n",
    "lr：学习率，在 Adam 优化器中设置为 0.001  （不变）\n",
    "\n",
    "损失函数：交叉熵损失和散度损失效果差不多，散度损失收敛速度可以更快，epoch200就可以了 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1602b219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['你好', '今天 天气 很 好', '今天 天气 很 好', '我 爱 学习', '我 喜欢 狗', '天气 很 好', '我 爱 养猫', '我 喜欢 学习', '你好', '今天 天气 很 好', '爱 养猫', '今天', '天气', '很', '好', '我', '爱', '学习', '我', '喜欢', '狗', '猫']\n",
      "['Hello', 'today weather very good', 'today weather very good', 'I love learning', 'I like dog', 'weather very good', 'I love cat', 'I like study', 'Hello', 'today weather very good', 'love cat', 'today', 'weather', 'very', 'good', 'I', 'love', 'learning', 'I', 'like', 'dog', 'cat']\n",
      "{'你好': 4, '今天': 5, '天气': 6, '很': 7, '好': 8, '我': 9, '爱': 10, '学习': 11, '喜欢': 12, '狗': 13, '养': 14, '猫': 15, '<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
      "{'Hello': 4, 'today': 5, 'weather': 6, 'very': 7, 'good': 8, 'I': 9, 'love': 10, 'learning': 11, 'like': 12, 'dog': 13, 'cat': 14, 'study': 15, '<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
      "[([1, 4, 2], [1, 4, 2]), ([1, 5, 6, 7, 8, 2], [1, 5, 6, 7, 8, 2]), ([1, 5, 6, 7, 8, 2], [1, 5, 6, 7, 8, 2]), ([1, 9, 10, 11, 2], [1, 9, 10, 11, 2]), ([1, 9, 12, 13, 2], [1, 9, 12, 13, 2]), ([1, 6, 7, 8, 2], [1, 6, 7, 8, 2]), ([1, 9, 10, 14, 2], [1, 9, 10, 14, 15, 2]), ([1, 9, 12, 15, 2], [1, 9, 12, 11, 2]), ([1, 4, 2], [1, 4, 2]), ([1, 5, 6, 7, 8, 2], [1, 5, 6, 7, 8, 2]), ([1, 10, 14, 2], [1, 10, 14, 15, 2]), ([1, 5, 2], [1, 5, 2]), ([1, 6, 2], [1, 6, 2]), ([1, 7, 2], [1, 7, 2]), ([1, 8, 2], [1, 8, 2]), ([1, 9, 2], [1, 9, 2]), ([1, 10, 2], [1, 10, 2]), ([1, 11, 2], [1, 11, 2]), ([1, 9, 2], [1, 9, 2]), ([1, 12, 2], [1, 12, 2]), ([1, 13, 2], [1, 13, 2]), ([1, 14, 2], [1, 15, 2])]\n",
      "Epoch 100, Loss: 0.00631779246032238\n",
      "Epoch 200, Loss: 0.0035212922375649214\n",
      "Training Finished\n",
      "Model have saved to ./model/mymodel_en2zh.pth\n",
      "Input: Hello -> Translated: 你好\n",
      "Input: today weather very good -> Translated: 今天 天气 很 好\n",
      "Input: I love dog -> Translated: 我 爱 狗\n",
      "Input: I like cat -> Translated: 我 喜欢 养 猫\n",
      "Input: love cat -> Translated: 爱 养 猫\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import spacy\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# 设置随机种子以确保可重复性\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 中文和英文句子\n",
    "chinese_sentences = [\n",
    "    \"你好\", \"今天 天气 很 好\",\n",
    "    \"今天 天气 很 好\",\n",
    "    \"我 爱 学习\", \"我 喜欢 狗\",\n",
    "    \"天气 很 好\", \"我 爱 养猫\", \"我 喜欢 学习\",\n",
    "    \"你好\", \"今天 天气 很 好\", \"爱 养猫\",\n",
    "    \"今天\", \"天气\", \"很\", \"好\",\n",
    "    \"我\", \"爱\", \"学习\", \"我\", \"喜欢\", \"狗\", \"猫\",\n",
    "]\n",
    "english_sentences = [\n",
    "    \"Hello\", \"today weather very good\",\n",
    "    \"today weather very good\",\n",
    "    \"I love learning\", \"I like dog\",\n",
    "    \"weather very good\", \"I love cat\", \"I like study\",\n",
    "    \"Hello\", \"today weather very good\", \"love cat\",\n",
    "    \"today\", \"weather\", \"very\", \"good\",\n",
    "    \"I\", \"love\", \"learning\", \"I\", \"like\", \"dog\", \"cat\",\n",
    "]\n",
    "\n",
    "print(chinese_sentences)\n",
    "print(english_sentences)\n",
    "\n",
    "\n",
    "# 加载 spacy 分词器\n",
    "spacy_ch = spacy.load('zh_core_web_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "# 分词函数\n",
    "def tokenize_ch(text):\n",
    "    return [tok.text for tok in spacy_ch.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# 构建词汇表\n",
    "\n",
    "def build_vocab(data, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for tokens in data:\n",
    "        counter.update(tokens)\n",
    "    vocab = {word: idx + 4 for idx, (word, freq) in enumerate(counter.items()) if freq >= min_freq}\n",
    "    vocab['<pad>'] = 0\n",
    "    vocab['<sos>'] = 1\n",
    "    vocab['<eos>'] = 2\n",
    "    vocab['<unk>'] = 3\n",
    "    return vocab\n",
    "\n",
    "# 构建中文和英文词汇表\n",
    "chinese_vocab = build_vocab([tokenize_ch(s) for s in chinese_sentences])\n",
    "english_vocab = build_vocab([tokenize_en(s) for s in english_sentences])\n",
    "\n",
    "def sentence_to_indices(sentence, vocab):\n",
    "    return [vocab['<sos>']] + [vocab.get(word, vocab['<unk>']) for word in sentence] + [vocab['<eos>']]\n",
    "\n",
    "# 将句子转换为索引序列\n",
    "data = [\n",
    "    (sentence_to_indices(tokenize_en(english), english_vocab),  # 英文句子和英文词汇表\n",
    "     sentence_to_indices(tokenize_ch(chinese), chinese_vocab)  # 中文句子和中文词汇表\n",
    "     )\n",
    "    for chinese, english in zip(chinese_sentences, english_sentences)\n",
    "]\n",
    "print(chinese_vocab)\n",
    "print(english_vocab)\n",
    "print(data)\n",
    "\n",
    "\n",
    "# 数据整理函数\n",
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    src_pad = pad_sequence([torch.tensor(s) for s in src_batch], padding_value=0, batch_first=False)\n",
    "    trg_pad = pad_sequence([torch.tensor(t) for t in trg_batch], padding_value=0, batch_first=False)\n",
    "    return src_pad, trg_pad\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "dataset = TranslationDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Transformer模型（修改输入输出维度的含义，代码结构不变）\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward,\n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward,\n",
    "                                          dropout, batch_first=False)\n",
    "        self.fc_out = nn.Linear(d_model, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _generate_positional_encoding(self, seq_len):\n",
    "        position = torch.arange(seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2, dtype=torch.float) * (-math.log(10000.0) / self.d_model))\n",
    "        pe = torch.zeros(seq_len, self.d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(1)  # [1, seq_len, d_model]\n",
    "\n",
    "    def forward(self, src, trg, trg_mask=None, padding_mask=None):\n",
    "        src_seq_length, N = src.shape\n",
    "        trg_seq_length, N = trg.shape\n",
    "        # 动态生成位置编码\n",
    "        src_pos = self._generate_positional_encoding(src_seq_length).to(src.device)\n",
    "        trg_pos = self._generate_positional_encoding(trg_seq_length).to(trg.device)\n",
    "        # 扩展位置编码的形状以匹配输入\n",
    "        src_pos = src_pos.expand(-1, N, -1)  # [1, seq_len, d_model] -> [seq_len, batch_size, d_model]\n",
    "        trg_pos = trg_pos.expand(-1, N, -1)  # [1, seq_len, d_model] -> [seq_len, batch_size, d_model]\n",
    "\n",
    "        src = self.dropout(self.embedding(src) + src_pos)\n",
    "        trg = self.dropout(self.embedding(trg) + trg_pos)\n",
    "        if trg_mask is None:\n",
    "            output = self.transformer(src, trg)\n",
    "        else:\n",
    "            output = self.transformer(src, trg, tgt_mask=trg_mask, tgt_key_padding_mask=padding_mask)\n",
    "        prediction = self.fc_out(output)\n",
    "        return prediction\n",
    "\n",
    "\n",
    "# 调整输入输出维度（源语言英文词汇表长度→输入，目标语言中文词汇表长度→输出）\n",
    "INPUT_DIM = len(english_vocab)  # 源语言（英文）词汇表大小\n",
    "OUTPUT_DIM = len(chinese_vocab)  # 目标语言（中文）词汇表大小\n",
    "D_MODEL = 64\n",
    "NHEAD = 4\n",
    "NUM_ENCODER_LAYERS = 2\n",
    "NUM_DECODER_LAYERS = 2\n",
    "DIM_FEEDFORWARD = 64\n",
    "DROPOUT = 0.05\n",
    "MAX_EPOCH = 200\n",
    "\n",
    "model = Transformer(INPUT_DIM, OUTPUT_DIM, D_MODEL, NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, DIM_FEEDFORWARD,\n",
    "                    DROPOUT)\n",
    "# print(model)\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.KLDivLoss(reduction='batchmean', log_target=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    for i, (src, trg) in enumerate(dataloader):\n",
    "        trg_mask = nn.Transformer.generate_square_subsequent_mask(trg.size(0) - 1).bool()\n",
    "        padding_mask = (trg[1:,] == 0).transpose(0, 1)\n",
    "        output = model(src, trg[:-1,], trg_mask=trg_mask, padding_mask=padding_mask)\n",
    "        # 将模型输出转换为对数概率分布\n",
    "        output_log_probs = nn.functional.log_softmax(output.view(-1, OUTPUT_DIM), dim=1)\n",
    "        # 将目标转换为 one - hot 编码\n",
    "        trg_one_hot = torch.nn.functional.one_hot(trg[1:,].view(-1), num_classes=OUTPUT_DIM).float()\n",
    "        loss = criterion(output_log_probs, trg_one_hot)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 99:\n",
    "        print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training Finished\")\n",
    "\n",
    "# 保存模型\n",
    "model_save_path = \"./model/mymodel_en2zh.pth\"\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model have saved to {model_save_path}\")\n",
    "\n",
    "\n",
    "# 翻译函数\n",
    "def translate_sentence(sentence, src_vocab, trg_vocab, model, max_len=50):\n",
    "    model.eval()\n",
    "    # 更改分词函数\n",
    "    tokens = tokenize_en(sentence)\n",
    "    indices = sentence_to_indices(tokens, src_vocab)\n",
    "    src_tensor = torch.tensor(indices).unsqueeze(1)\n",
    "    # src_len=torch.tensor(len(indices)).unsqueeze(0)\n",
    "    # print(\"src_tensor:\",src_tensor)\n",
    "    trg_indices = [trg_vocab['<sos>']]  # 目标语言以<sos>开始\n",
    "\n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.tensor(trg_indices).unsqueeze(1)\n",
    "        with torch.no_grad():\n",
    "            output = model(src_tensor, trg_tensor)\n",
    "        pred_token = output.argmax(2)[-1].item()\n",
    "        trg_indices.append(pred_token)\n",
    "        if pred_token == trg_vocab['<eos>']:\n",
    "            break\n",
    "\n",
    "    # 转换为目标语言（中文）tokens\n",
    "    trg_tokens = [list(trg_vocab.keys())[list(trg_vocab.values()).index(i)] for i in trg_indices]\n",
    "    final_tokens = [token for token in trg_tokens if token not in ['<sos>', '<eos>']]\n",
    "    return ' '.join(final_tokens)\n",
    "\n",
    "# 加载模型（结构不变）\n",
    "loaded_model = Transformer(INPUT_DIM, OUTPUT_DIM, D_MODEL, NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, DIM_FEEDFORWARD,\n",
    "                           DROPOUT)\n",
    "loaded_model.load_state_dict(torch.load(model_save_path, weights_only=True))\n",
    "\n",
    "# 测试翻译（输入英文句子）\n",
    "english_test_sentences = [\"Hello\", \"today weather very good\", \"I love dog\", \"I like cat\", \"love cat\"]\n",
    "for sentence in english_test_sentences:\n",
    "    translation = translate_sentence(sentence, english_vocab, chinese_vocab, loaded_model)\n",
    "    print(f'Input: {sentence} -> Translated: {translation}')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装所需的依赖库\n",
    "%pip install spacy==3.8.0\n",
    "# [选择1]如果网络允许, 通过以下命令可以直接下载中英文分词器\n",
    "# !python -m spacy download zh_core_web_sm\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [选择2]通过本地文件安装中英文分词器\n",
    "%pip install ./en_core_web_sm-3.8.0.tar.gz\n",
    "# 由于安装中文分词器时, 会强制改变numpy版本产生兼容性问题, 这边需要加入--no-deps, 表示不改变额外的依赖库\n",
    "%pip install --no-deps ./zh_core_web_sm-3.8.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于前面安装了新的库, 执行下面代码块前可能需要重启内核\n",
    "# 导入依赖库\n",
    "import torch\n",
    "import spacy\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['你好', '今天 天气 很 好', '今天 天气 很 好', '我 爱 学习', '我 喜欢 狗', '天气 很 好', '我 爱 养猫', '我 喜欢 学习', '你好', '今天 天气 很 好', '爱 养猫今天', '天气', '很', '好', '我', '爱', '学习', '我', '喜欢', '狗', '猫']\n",
      "['Hello', 'today weather very good', 'today weather very good', 'I love learning', 'I like dog', 'weather very good', 'I love cat', 'I like study', 'Hello', 'today weather very good', 'love cattoday', 'weather', 'very', 'good', 'I', 'love', 'learning', 'I', 'like', 'dog', 'cat']\n"
     ]
    }
   ],
   "source": [
    "# 设置随机种子以确保可重复性\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 中文和英文句子\n",
    "chinese_sentences = [ \"你好\", \"今天 天气 很 好\",\n",
    "                     \"今天 天气 很 好\",\n",
    "                     \"我 爱 学习\",\"我 喜欢 狗\",\n",
    "                     \"天气 很 好\",\"我 爱 养猫\",\"我 喜欢 学习\",\n",
    "                     \"你好\", \"今天 天气 很 好\",\"爱 养猫\"\n",
    "                     \"今天\", \"天气\", \"很\", \"好\",\n",
    "                     \"我\", \"爱\", \"学习\",\"我\",\"喜欢\",\"狗\",\"猫\",\n",
    "                     ]\n",
    "english_sentences = [ \"Hello\", \"today weather very good\",\n",
    "                     \"today weather very good\",\n",
    "                     \"I love learning\",\"I like dog\",\n",
    "                     \"weather very good\",\"I love cat\",\"I like study\",\n",
    "                     \"Hello\", \"today weather very good\",\"love cat\"\n",
    "                     \"today\", \"weather\", \"very\", \"good\",\n",
    "                     \"I\", \"love\", \"learning\",\"I\",\"like\",\"dog\",\"cat\",\n",
    "                     ]\n",
    "\n",
    "print(chinese_sentences)\n",
    "print(english_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'你好': 4, '今天': 5, '天气': 6, '很': 7, '好': 8, '我': 9, '爱': 10, '学习': 11, '喜欢': 12, '狗': 13, '养': 14, '猫': 15, '养猫': 16, '<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
      "{'Hello': 4, 'today': 5, 'weather': 6, 'very': 7, 'good': 8, 'I': 9, 'love': 10, 'learning': 11, 'like': 12, 'dog': 13, 'cat': 14, 'study': 15, 'cattoday': 16, '<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
      "[([1, 4, 2], [1, 4, 2]), ([1, 5, 6, 7, 8, 2], [1, 5, 6, 7, 8, 2]), ([1, 5, 6, 7, 8, 2], [1, 5, 6, 7, 8, 2]), ([1, 9, 10, 11, 2], [1, 9, 10, 11, 2]), ([1, 9, 12, 13, 2], [1, 9, 12, 13, 2]), ([1, 6, 7, 8, 2], [1, 6, 7, 8, 2]), ([1, 9, 10, 14, 15, 2], [1, 9, 10, 14, 2]), ([1, 9, 12, 11, 2], [1, 9, 12, 15, 2]), ([1, 4, 2], [1, 4, 2]), ([1, 5, 6, 7, 8, 2], [1, 5, 6, 7, 8, 2]), ([1, 10, 16, 5, 2], [1, 10, 16, 2]), ([1, 6, 2], [1, 6, 2]), ([1, 7, 2], [1, 7, 2]), ([1, 8, 2], [1, 8, 2]), ([1, 9, 2], [1, 9, 2]), ([1, 10, 2], [1, 10, 2]), ([1, 11, 2], [1, 11, 2]), ([1, 9, 2], [1, 9, 2]), ([1, 12, 2], [1, 12, 2]), ([1, 13, 2], [1, 13, 2]), ([1, 15, 2], [1, 14, 2])]\n"
     ]
    }
   ],
   "source": [
    "# 加载 spacy 分词器\n",
    "spacy_ch = spacy.load('zh_core_web_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "# 分词函数\n",
    "def tokenize_ch(text):\n",
    "    return [tok.text for tok in spacy_ch.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# 构建词汇表\n",
    "\n",
    "def build_vocab(data, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for tokens in data:\n",
    "        counter.update(tokens)\n",
    "    vocab = {word: idx + 4 for idx, (word, freq) in enumerate(counter.items()) if freq >= min_freq}\n",
    "    vocab['<pad>'] = 0\n",
    "    vocab['<sos>'] = 1\n",
    "    vocab['<eos>'] = 2\n",
    "    vocab['<unk>'] = 3\n",
    "    return vocab\n",
    "\n",
    "# 构建中文和英文词汇表\n",
    "chinese_vocab = build_vocab([tokenize_ch(s) for s in chinese_sentences])\n",
    "english_vocab = build_vocab([tokenize_en(s) for s in english_sentences])\n",
    "\n",
    "def sentence_to_indices(sentence, vocab):\n",
    "    return [vocab['<sos>']] + [vocab.get(word, vocab['<unk>']) for word in sentence] + [vocab['<eos>']]\n",
    "\n",
    "# 将句子转换为索引序列\n",
    "data = [\n",
    "    (sentence_to_indices(tokenize_ch(chinese), chinese_vocab),  # 中文句子和中文词汇表\n",
    "    sentence_to_indices(tokenize_en(english), english_vocab)   # 英文句子和英文词汇表\n",
    "    )\n",
    "    for chinese, english in zip(chinese_sentences, english_sentences)\n",
    "]\n",
    "print(chinese_vocab)\n",
    "print(english_vocab)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "     \n",
    "    # 填充批次中的句子\n",
    "    src_pad = pad_sequence([torch.tensor(s) for s in src_batch], padding_value=0,batch_first=False)\n",
    "    trg_pad = pad_sequence([torch.tensor(t) for t in trg_batch], padding_value=0,batch_first=False)\n",
    "    \n",
    "    return src_pad, trg_pad\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "dataset = TranslationDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout, batch_first=False)\n",
    "        self.fc_out = nn.Linear(d_model, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def _generate_positional_encoding(self, seq_len):\n",
    "        position = torch.arange(seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2, dtype=torch.float) * (-math.log(10000.0) / self.d_model))\n",
    "        pe = torch.zeros(seq_len, self.d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(1)  # [1, seq_len, d_model]\n",
    "\n",
    "\n",
    "    def forward(self, src, trg, trg_mask = None, padding_mask=None):\n",
    "        src_seq_length,N = src.shape\n",
    "        trg_seq_length,N = trg.shape\n",
    "        \n",
    "        # 动态生成位置编码\n",
    "        src_pos = self._generate_positional_encoding(src_seq_length).to(src.device)\n",
    "        trg_pos = self._generate_positional_encoding(trg_seq_length).to(trg.device)\n",
    "       # 扩展位置编码的形状以匹配输入\n",
    "        src_pos = src_pos.expand(-1, N, -1)  # [1, seq_len, d_model] -> [seq_len, batch_size, d_model]\n",
    "        trg_pos = trg_pos.expand(-1, N, -1)  # [1, seq_len, d_model] -> [seq_len, batch_size, d_model]\n",
    "        \n",
    "        src = self.dropout(self.embedding(src) + src_pos)\n",
    "        trg = self.dropout(self.embedding(trg) + trg_pos)\n",
    "        if(trg_mask == None):\n",
    "            output = self.transformer(src, trg)\n",
    "        else:\n",
    "            output = self.transformer(src, trg, tgt_mask=trg_mask,tgt_key_padding_mask=padding_mask)\n",
    "        prediction = self.fc_out(output)\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python 3.11.5\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 0.06975290924310684\n",
      "Epoch 200, Loss: 0.016102923080325127\n",
      "Training Finished\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(chinese_vocab)\n",
    "OUTPUT_DIM = len(english_vocab)\n",
    "D_MODEL = 32\n",
    "NHEAD = 2\n",
    "NUM_ENCODER_LAYERS = 2\n",
    "NUM_DECODER_LAYERS = 2\n",
    "DIM_FEEDFORWARD = 32\n",
    "DROPOUT = 0.05\n",
    "MAX_EPOCH = 200\n",
    "\n",
    "model = Transformer(INPUT_DIM, OUTPUT_DIM, D_MODEL, NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, DIM_FEEDFORWARD, DROPOUT)\n",
    "#print(model)\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    # for src, trg in dataloader:\n",
    "    for i, (src, trg) in enumerate(dataloader):\n",
    "        \n",
    "        trg_mask = nn.Transformer.generate_square_subsequent_mask(trg.size(0)-1).bool() # 目标序列掩码\n",
    "        #print(trg_mask)\n",
    "        padding_mask = (trg[1:,] == 0 ).transpose(0, 1) # (N, T)\n",
    "        #print(padding_mask)\n",
    "        output = model(src, trg[:-1,], trg_mask=trg_mask, padding_mask=padding_mask)  # 去掉最后一个token:trg[:-1,]\n",
    "        #print(\"output.shape:\",output.shape)\n",
    "        #if epoch == 1000:\n",
    "            #print(\"trg:\", trg.T.flatten())\n",
    "            #pred_token = output.argmax(2)\n",
    "            #print(\"pred_token\",pred_token.T.flatten())\n",
    "        loss = criterion(output.view(-1, OUTPUT_DIM), trg[1:,].view(-1))  # 去掉第一个 token\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()     \n",
    "    \n",
    "    if epoch % 100 == 99:     \n",
    "        print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model have saved to ./model/mymodel.pth\n"
     ]
    }
   ],
   "source": [
    "# 创建目录, 保存模型\n",
    "model_save_path = \"./model/mymodel.pth\"\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model have saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated sentence: Hello\n",
      "Translated sentence: weather very good\n",
      "Translated sentence: cat\n",
      "Translated sentence: I like study\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python 3.11.5\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def translate_sentence(sentence, src_vocab, trg_vocab, model, max_len=50):\n",
    "    model.eval()\n",
    "    tokens = tokenize_ch(sentence)\n",
    "    indices = sentence_to_indices(tokens, src_vocab)    \n",
    "    src_tensor = torch.tensor(indices).unsqueeze(1)\n",
    "    #src_len=torch.tensor(len(indices)).unsqueeze(0)\n",
    "    #print(\"src_tensor:\",src_tensor)\n",
    "    trg_indices = [trg_vocab['<sos>']]\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.tensor(trg_indices).unsqueeze(1)\n",
    "        with torch.no_grad():\n",
    "            output = model(src_tensor, trg_tensor)\n",
    "            #print(\"output:\",output.argmax(2))\n",
    "        pred_token = output.argmax(2)[-1].item()\n",
    "        #print(\"pred:\",pred_token)\n",
    "        trg_indices.append(pred_token)\n",
    "        if pred_token == trg_vocab['<eos>']:\n",
    "            break\n",
    "    \n",
    "    trg_tokens = [list(trg_vocab.keys())[list(trg_vocab.values()).index(i)] for i in trg_indices]\n",
    "    # 过滤<sos>和<eos>\n",
    "    final_tokens = [token for token in trg_tokens if token not in ['<sos>', '<eos>']]\n",
    "\n",
    "    return ' '.join(final_tokens[:])\n",
    "\n",
    "# 加载模型\n",
    "model_load_path = \"./model/mymodel.pth\"\n",
    "loaded_model = Transformer(INPUT_DIM, OUTPUT_DIM, D_MODEL, NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, DIM_FEEDFORWARD, DROPOUT)\n",
    "loaded_model.load_state_dict(torch.load(model_load_path, weights_only=True))\n",
    "\n",
    "chinese_sentences = [ \"你好\", \"天气 很 好\",\"猫\", \"我 喜欢 学习\"]\n",
    "# 测试翻译\n",
    "for sentence in chinese_sentences:\n",
    "    translation = translate_sentence(sentence, chinese_vocab, english_vocab, model)\n",
    "    print(f'Translated sentence: {translation}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
